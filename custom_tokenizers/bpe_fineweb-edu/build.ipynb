{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2351bdee-ae93-4140-a11a-aacfb1254bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that. \n",
    "# you prolly won't need this cell but running it won't hurt anything either\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) \n",
    "\n",
    "# so that we can import `tools.py`\n",
    "# Navigate two directories up from the current working directory\n",
    "module_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "# Add this directory to sys.path if it's not already included\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba617fb-d786-4169-8d39-2a97145c9245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a145fd56-b6b4-4188-b9c2-ef0cb928ddde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28dc2e6e3f4142efad699cd162fdc224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d190d1388ec4bcb90e826e11a053294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# loading a sample from the dataset specified in config, in this case 'HuggingFaceFW/fineweb'\n",
    "from tools import get_data_loaders\n",
    "train_data_loader, val_data_loader = get_data_loaders(\n",
    "    'HuggingFaceFW/fineweb-edu', \n",
    "    batch_size=1, \n",
    "    streaming=True,\n",
    "    subset_name = \"CC-MAIN-2024-10\" # using a different subset of fineweb from what we'll be training the model on\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c7ed4-f97c-4440-abea-c027216e9aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "batch = next(train_data_loader)\n",
    "print(len(batch), batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2133207-6ee0-4f47-8018-da1a867c5136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446616\n",
      "For those unfamiliar with Cornish, it is classed as a p-Celtic member of the family of Celtic langua\n",
      "[70, 111, 114, 32, 116, 104, 111, 115, 101, 32, 117, 110, 102, 97, 109, 105, 108, 105, 97, 114, 32, 119, 105, 116, 104, 32, 67, 111, 114, 110, 105, 115, 104, 44, 32, 105, 116, 32, 105, 115, 32, 99, 108, 97, 115, 115, 101, 100, 32, 97, 115, 32, 97, 32, 112, 45, 67, 101, 108, 116, 105, 99, 32, 109, 101, 109, 98, 101, 114, 32, 111, 102, 32, 116, 104, 101, 32, 102, 97, 109, 105, 108, 121, 32, 111, 102, 32, 67, 101, 108, 116, 105, 99, 32, 108, 97, 110, 103, 117, 97]\n",
      "For those unfamiliar with Cornish, it is classed as a p-Celtic member of the family of Celtic langua\n"
     ]
    }
   ],
   "source": [
    "# turn it into one string instead of a list of strings\n",
    "combined_string = '\\n\\n'.join(batch)\n",
    "\n",
    "# Convert the string to bytes\n",
    "combined_bytes = combined_string.encode('utf-8')\n",
    "print(len(combined_bytes))\n",
    "\n",
    "print(combined_string[:100])\n",
    "print([b for b in combined_bytes[:100]])\n",
    "print(combined_bytes.decode('utf-8')[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca80d95-7491-4ffb-9bbc-63e521306590",
   "metadata": {},
   "source": [
    "# Regex\n",
    "this is a pre-processing stage where we set the rules for what types of characters are allowed to be merged together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5da62b54-17dd-48a0-bab4-f9b1fd07413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d3dad59-9791-4588-96c3-134a2d8390a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't ask me the specifics of how this plays out, i just know it's what they used for GPT4\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "# if you want to mess around with building your own tokenizer, then ^this string is one of the things to mess around with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c04f316b-21a9-4bdc-a7ed-c622eb1b4e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex.Regex(\"'(?i:[sdmt]|ll|ve|re)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?+\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]++[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]|\\\\s+(?!\\\\S)|\\\\s+\", flags=regex.V0)\n"
     ]
    }
   ],
   "source": [
    "compiled_pattern = re.compile(GPT4_SPLIT_PATTERN, re.UNICODE)\n",
    "print(compiled_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c183a0fc-400d-41ed-9272-1d7a96e08247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1056254 207679\n"
     ]
    }
   ],
   "source": [
    "# split the text up into text chunks\n",
    "text_chunks = re.findall(compiled_pattern, combined_string)\n",
    "print(len(combined_string), len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dad461b-9959-463e-9a87-8934400ecb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Independent Jane\n",
      "For all the love, romance and scandal in Jane Austen’s books, what they are rea\n",
      "['The', ' Independent', ' Jane', '\\n', 'For', ' all', ' the', ' love', ',', ' romance', ' and', ' scandal', ' in', ' Jane', ' Austen', '’s', ' books', ',', ' what', ' they', ' are', ' really', ' about', ' is', ' freedom', ' and', ' independence', '.', ' Independence', ' of', ' thought', ' and', ' the', ' freedom', ' to', ' choose', '.\\n', 'Elizabeth', '’s', ' refusal', ' of', ' Mr', '.', ' Collins', ' offer', ' of', ' marriage', ' showed', ' an', ' independence', ' seldom', ' seen', ' in', ' heroines', ' of', ' the', ' day', '.', ' Her', ' refusal', ' of', ' Mr', '.', ' Darcy', ' while', ' triggered', ' by', ' anger', ' showed', ' a', ' level', ' of', ' independence', ' that', ' left', ' him', ' shocked', ' and', ' stunned', '.\\n', 'The', ' freedom', ' she', ' exhibited', ' in', ' finally', ' accepting', ' him', ' in', ' direct', ' defiance', ' of', ' Lady', ' Catherine', ' and', ' knowing', ' her', ' father', ' would', ' disapprove']\n"
     ]
    }
   ],
   "source": [
    "print(combined_string[:100])\n",
    "print(text_chunks[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa9c183-5eb1-4b32-a5f7-3e5813b1e100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207679 [[84, 104, 101], [32, 73, 110, 100, 101, 112, 101, 110, 100, 101, 110, 116], [32, 74, 97, 110, 101], [10], [70, 111, 114], [32, 97, 108, 108], [32, 116, 104, 101], [32, 108, 111, 118, 101], [44], [32, 114, 111, 109, 97, 110, 99, 101], [32, 97, 110, 100], [32, 115, 99, 97, 110, 100, 97, 108], [32, 105, 110], [32, 74, 97, 110, 101], [32, 65, 117, 115, 116, 101, 110], [226, 128, 153, 115], [32, 98, 111, 111, 107, 115], [44], [32, 119, 104, 97, 116], [32, 116, 104, 101, 121], [32, 97, 114, 101], [32, 114, 101, 97, 108, 108, 121], [32, 97, 98, 111, 117, 116], [32, 105, 115], [32, 102, 114, 101, 101, 100, 111, 109], [32, 97, 110, 100], [32, 105, 110, 100, 101, 112, 101, 110, 100, 101, 110, 99, 101], [46], [32, 73, 110, 100, 101, 112, 101, 110, 100, 101, 110, 99, 101], [32, 111, 102], [32, 116, 104, 111, 117, 103, 104, 116], [32, 97, 110, 100], [32, 116, 104, 101], [32, 102, 114, 101, 101, 100, 111, 109], [32, 116, 111], [32, 99, 104, 111, 111, 115, 101], [46, 10], [69, 108, 105, 122, 97, 98, 101, 116, 104], [226, 128, 153, 115], [32, 114, 101, 102, 117, 115, 97, 108], [32, 111, 102], [32, 77, 114], [46], [32, 67, 111, 108, 108, 105, 110, 115], [32, 111, 102, 102, 101, 114], [32, 111, 102], [32, 109, 97, 114, 114, 105, 97, 103, 101], [32, 115, 104, 111, 119, 101, 100], [32, 97, 110], [32, 105, 110, 100, 101, 112, 101, 110, 100, 101, 110, 99, 101], [32, 115, 101, 108, 100, 111, 109], [32, 115, 101, 101, 110], [32, 105, 110], [32, 104, 101, 114, 111, 105, 110, 101, 115], [32, 111, 102], [32, 116, 104, 101], [32, 100, 97, 121], [46], [32, 72, 101, 114], [32, 114, 101, 102, 117, 115, 97, 108], [32, 111, 102], [32, 77, 114], [46], [32, 68, 97, 114, 99, 121], [32, 119, 104, 105, 108, 101], [32, 116, 114, 105, 103, 103, 101, 114, 101, 100], [32, 98, 121], [32, 97, 110, 103, 101, 114], [32, 115, 104, 111, 119, 101, 100], [32, 97], [32, 108, 101, 118, 101, 108], [32, 111, 102], [32, 105, 110, 100, 101, 112, 101, 110, 100, 101, 110, 99, 101], [32, 116, 104, 97, 116], [32, 108, 101, 102, 116], [32, 104, 105, 109], [32, 115, 104, 111, 99, 107, 101, 100], [32, 97, 110, 100], [32, 115, 116, 117, 110, 110, 101, 100], [46, 10], [84, 104, 101], [32, 102, 114, 101, 101, 100, 111, 109], [32, 115, 104, 101], [32, 101, 120, 104, 105, 98, 105, 116, 101, 100], [32, 105, 110], [32, 102, 105, 110, 97, 108, 108, 121], [32, 97, 99, 99, 101, 112, 116, 105, 110, 103], [32, 104, 105, 109], [32, 105, 110], [32, 100, 105, 114, 101, 99, 116], [32, 100, 101, 102, 105, 97, 110, 99, 101], [32, 111, 102], [32, 76, 97, 100, 121], [32, 67, 97, 116, 104, 101, 114, 105, 110, 101], [32, 97, 110, 100], [32, 107, 110, 111, 119, 105, 110, 103], [32, 104, 101, 114], [32, 102, 97, 116, 104, 101, 114], [32, 119, 111, 117, 108, 100], [32, 100, 105, 115, 97, 112, 112, 114, 111, 118, 101]]\n"
     ]
    }
   ],
   "source": [
    "# input text preprocessing\n",
    "ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks] \n",
    "ids_backup = ids # saving this for later just to see how much compression we get\n",
    "print(len(ids), ids[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b53373-b33d-4eba-b640-a50e3334045a",
   "metadata": {},
   "source": [
    "so this regex just splits the text up into all the token ids that are allowed to be merged, meaning that the regex output we saw above is an upper limit on the tokens that we could end up with if we get a large enough vocabulary, rather than a starting point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e216a550-4bbb-4d8e-ba57-25af48a9d65b",
   "metadata": {},
   "source": [
    "# BPE tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba69cd54-8390-4355-881a-cd2f233922dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1024 - 3 # the desired final vocabulary size. -3 for the three special tokens bos, eos, & pad\n",
    "num_merges = vocab_size - 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0e7fcb6-5927-45b9-b6a9-23ce1df8f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids, counts=None):\n",
    "    \"\"\"\n",
    "    Given a list of integers, return a dictionary of counts of consecutive pairs\n",
    "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
    "    Optionally allows to update an existing dictionary of counts\n",
    "    \"\"\"\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids, ids[1:]): # iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    \"\"\"\n",
    "    In the list of integers (ids), replace all consecutive occurrences\n",
    "    of pair with the new integer token idx\n",
    "    Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
    "    \"\"\"\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if not at the very last position AND the pair matches, replace it\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba933bfb-026c-47f7-8fff-42868bf5818c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 765/765 [03:15<00:00,  3.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# now let's actually do it\n",
    "merges = {} # (int, int) -> int\n",
    "for i in tqdm(range(num_merges)):\n",
    "    # count the number of times every consecutive pair appears\n",
    "    stats = {}\n",
    "    for chunk_ids in ids:\n",
    "        # passing in stats will update it in place, adding up counts\n",
    "        get_stats(chunk_ids, stats)\n",
    "    # find the pair with the highest count\n",
    "    pair = max(stats, key=stats.get)\n",
    "    # mint a new token: assign it the next available id\n",
    "    idx = 256 + i\n",
    "    # replace all occurrences of pair in ids with idx\n",
    "    ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]\n",
    "    # save the merge\n",
    "    merges[pair] = idx\n",
    "    #print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} had {stats[pair]} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ea99684-f232-4a8f-80f4-48b841633ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original length: 1060598\n",
      "ids length: 417949\n",
      "compression ratio: 2.54X\n"
     ]
    }
   ],
   "source": [
    "og = sum([len(t) for t in ids_backup])\n",
    "new = sum([len(t) for t in (ids)])\n",
    "print(\"original length:\", og)\n",
    "print(\"ids length:\", new)\n",
    "print(f\"compression ratio: {og / new:.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bffaa27-acf6-4173-8f64-44f8def4a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[456] The\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    # given ids (list of integers), return Python string\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text\n",
    "\n",
    "print(ids[0], decode(ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad46ab33-fa0a-4c88-8779-27387f3fceb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79, 110, 317, 596, 265, 257, 662, 44]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    # given a string, return list of integers (the tokens)\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break # nothing else can be merged\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "print(encode(\"Once upon a time,\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcfc996d-fc57-469d-9a25-bc441cc4242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the tokenizers directory exists\n",
    "if not os.path.exists('./models'):\n",
    "    os.makedirs('./models')\n",
    "\n",
    "# Prepare the tokenizer data to be saved\n",
    "tokenizer_data = {'merges': merges}\n",
    "\n",
    "# Save the tokenizer data using pickle\n",
    "with open(f'./models/{vocab_size}.model', 'wb') as f:\n",
    "    pickle.dump(tokenizer_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db232b16-94c5-4a41-ba7f-63c26bcfff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a pre-existing tokenizer and trimming it down to a smaller size\n",
    "# i basically ran this cell and then one above it multiple times until i got to the smallest possible size (512 - 3 = 509)\n",
    "vocab_size = ((vocab_size + 3) // 2) - 3 # the -3's account for our special tokens\n",
    "merges = {k: v for k, v in merges.items() if v < vocab_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "030522f4-ad67-4200-8598-261a9e1fa744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import load_tokenizer_data, BPE_Tokenizer\n",
    "vocab_size = 1024\n",
    "tokenizer_data = load_tokenizer_data(f'models/{vocab_size-3}.model')\n",
    "tokenizer = BPE_Tokenizer(tokenizer_data['merges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "857e3d5b-85b9-4ce7-bd41-f1f8de91cb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', ' b', 'est', ' and', ' most', ' rel', 'i', 'able', ' form', ' of', ' rese', 'arch', ' is', ' the', ' d', 'ou', 'b', 'le', '-', 'b', 'l', 'ind', ',', ' pl', 'ace', 'b', 'o', '-', 'con', 't', 'rol', 'led', ' stud', 'y', '.']\n",
      "[1021, 456, 278, 402, 285, 746, 808, 105, 550, 848, 277, 969, 953, 311, 262, 288, 286, 98, 292, 45, 98, 108, 608, 44, 458, 613, 98, 111, 45, 659, 116, 869, 723, 602, 121, 46]\n",
      "The best and most reliable form of research is the double-blind, placebo-controlled study.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'The best and most reliable form of research is the double-blind, placebo-controlled study.'\n",
    "print(tokenizer.display(prompt))\n",
    "\n",
    "tokens = tokenizer.encode(prompt)\n",
    "print(tokens)\n",
    "\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba18a30-c97c-41bc-a37b-23e82bd7d055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: '\u0000'\n",
      "1: '\u0001'\n",
      "2: '\u0002'\n",
      "3: '\u0003'\n",
      "4: '\u0004'\n",
      "5: '\u0005'\n",
      "6: '\u0006'\n",
      "7: '\u0007'\n",
      "8: '\n",
      "9: '\t'\n",
      "10: '\n",
      "'\n",
      "11: '\u000b",
      "'\n",
      "12: '\f",
      "'\n",
      "'3: '\n",
      "14: '\u000e'\n",
      "15: '\u000f'\n",
      "16: '\u0010'\n",
      "17: '\u0011'\n",
      "18: '\u0012'\n",
      "19: '\u0013'\n",
      "20: '\u0014'\n",
      "21: '\u0015'\n",
      "22: '\u0016'\n",
      "23: '\u0017'\n",
      "24: '\u0018'\n",
      "25: '\u0019'\n",
      "26: '\u001a'\n",
      "27: '\u001b'\n",
      "28: '\u001c",
      "'\n",
      "29: '\u001d",
      "'\n",
      "30: '\u001e",
      "'\n",
      "31: '\u001f'\n",
      "32: ' '\n",
      "33: '!'\n",
      "34: '\"'\n",
      "35: '#'\n",
      "36: '$'\n",
      "37: '%'\n",
      "38: '&'\n",
      "39: '''\n",
      "40: '('\n",
      "41: ')'\n",
      "42: '*'\n",
      "43: '+'\n",
      "44: ','\n",
      "45: '-'\n",
      "46: '.'\n",
      "47: '/'\n",
      "48: '0'\n",
      "49: '1'\n",
      "50: '2'\n",
      "51: '3'\n",
      "52: '4'\n",
      "53: '5'\n",
      "54: '6'\n",
      "55: '7'\n",
      "56: '8'\n",
      "57: '9'\n",
      "58: ':'\n",
      "59: ';'\n",
      "60: '<'\n",
      "61: '='\n",
      "62: '>'\n",
      "63: '?'\n",
      "64: '@'\n",
      "65: 'A'\n",
      "66: 'B'\n",
      "67: 'C'\n",
      "68: 'D'\n",
      "69: 'E'\n",
      "70: 'F'\n",
      "71: 'G'\n",
      "72: 'H'\n",
      "73: 'I'\n",
      "74: 'J'\n",
      "75: 'K'\n",
      "76: 'L'\n",
      "77: 'M'\n",
      "78: 'N'\n",
      "79: 'O'\n",
      "80: 'P'\n",
      "81: 'Q'\n",
      "82: 'R'\n",
      "83: 'S'\n",
      "84: 'T'\n",
      "85: 'U'\n",
      "86: 'V'\n",
      "87: 'W'\n",
      "88: 'X'\n",
      "89: 'Y'\n",
      "90: 'Z'\n",
      "91: '['\n",
      "92: '\\'\n",
      "93: ']'\n",
      "94: '^'\n",
      "95: '_'\n",
      "96: '`'\n",
      "97: 'a'\n",
      "98: 'b'\n",
      "99: 'c'\n",
      "100: 'd'\n",
      "101: 'e'\n",
      "102: 'f'\n",
      "103: 'g'\n",
      "104: 'h'\n",
      "105: 'i'\n",
      "106: 'j'\n",
      "107: 'k'\n",
      "108: 'l'\n",
      "109: 'm'\n",
      "110: 'n'\n",
      "111: 'o'\n",
      "112: 'p'\n",
      "113: 'q'\n",
      "114: 'r'\n",
      "115: 's'\n",
      "116: 't'\n",
      "117: 'u'\n",
      "118: 'v'\n",
      "119: 'w'\n",
      "120: 'x'\n",
      "121: 'y'\n",
      "122: 'z'\n",
      "123: '{'\n",
      "124: '|'\n",
      "125: '}'\n",
      "126: '~'\n",
      "127: ''\n",
      "128: '�'\n",
      "129: '�'\n",
      "130: '�'\n",
      "131: '�'\n",
      "132: '�'\n",
      "133: '�'\n",
      "134: '�'\n",
      "135: '�'\n",
      "136: '�'\n",
      "137: '�'\n",
      "138: '�'\n",
      "139: '�'\n",
      "140: '�'\n",
      "141: '�'\n",
      "142: '�'\n",
      "143: '�'\n",
      "144: '�'\n",
      "145: '�'\n",
      "146: '�'\n",
      "147: '�'\n",
      "148: '�'\n",
      "149: '�'\n",
      "150: '�'\n",
      "151: '�'\n",
      "152: '�'\n",
      "153: '�'\n",
      "154: '�'\n",
      "155: '�'\n",
      "156: '�'\n",
      "157: '�'\n",
      "158: '�'\n",
      "159: '�'\n",
      "160: '�'\n",
      "161: '�'\n",
      "162: '�'\n",
      "163: '�'\n",
      "164: '�'\n",
      "165: '�'\n",
      "166: '�'\n",
      "167: '�'\n",
      "168: '�'\n",
      "169: '�'\n",
      "170: '�'\n",
      "171: '�'\n",
      "172: '�'\n",
      "173: '�'\n",
      "174: '�'\n",
      "175: '�'\n",
      "176: '�'\n",
      "177: '�'\n",
      "178: '�'\n",
      "179: '�'\n",
      "180: '�'\n",
      "181: '�'\n",
      "182: '�'\n",
      "183: '�'\n",
      "184: '�'\n",
      "185: '�'\n",
      "186: '�'\n",
      "187: '�'\n",
      "188: '�'\n",
      "189: '�'\n",
      "190: '�'\n",
      "191: '�'\n",
      "192: '�'\n",
      "193: '�'\n",
      "194: '�'\n",
      "195: '�'\n",
      "196: '�'\n",
      "197: '�'\n",
      "198: '�'\n",
      "199: '�'\n",
      "200: '�'\n",
      "201: '�'\n",
      "202: '�'\n",
      "203: '�'\n",
      "204: '�'\n",
      "205: '�'\n",
      "206: '�'\n",
      "207: '�'\n",
      "208: '�'\n",
      "209: '�'\n",
      "210: '�'\n",
      "211: '�'\n",
      "212: '�'\n",
      "213: '�'\n",
      "214: '�'\n",
      "215: '�'\n",
      "216: '�'\n",
      "217: '�'\n",
      "218: '�'\n",
      "219: '�'\n",
      "220: '�'\n",
      "221: '�'\n",
      "222: '�'\n",
      "223: '�'\n",
      "224: '�'\n",
      "225: '�'\n",
      "226: '�'\n",
      "227: '�'\n",
      "228: '�'\n",
      "229: '�'\n",
      "230: '�'\n",
      "231: '�'\n",
      "232: '�'\n",
      "233: '�'\n",
      "234: '�'\n",
      "235: '�'\n",
      "236: '�'\n",
      "237: '�'\n",
      "238: '�'\n",
      "239: '�'\n",
      "240: '�'\n",
      "241: '�'\n",
      "242: '�'\n",
      "243: '�'\n",
      "244: '�'\n",
      "245: '�'\n",
      "246: '�'\n",
      "247: '�'\n",
      "248: '�'\n",
      "249: '�'\n",
      "250: '�'\n",
      "251: '�'\n",
      "252: '�'\n",
      "253: '�'\n",
      "254: '�'\n",
      "255: '�'\n",
      "256: ' t'\n",
      "257: ' a'\n",
      "258: 'he'\n",
      "259: 'in'\n",
      "260: 're'\n",
      "261: ' o'\n",
      "262: ' the'\n",
      "263: 'er'\n",
      "264: 'at'\n",
      "265: 'on'\n",
      "266: ' s'\n",
      "267: ' w'\n",
      "268: 'nd'\n",
      "269: ' c'\n",
      "270: 'es'\n",
      "271: 'is'\n",
      "272: 'en'\n",
      "273: ' p'\n",
      "274: 'or'\n",
      "275: 'ed'\n",
      "276: 'it'\n",
      "277: ' of'\n",
      "278: ' b'\n",
      "279: 'an'\n",
      "280: 'al'\n",
      "281: ' f'\n",
      "282: 'ing'\n",
      "283: ' in'\n",
      "284: ' to'\n",
      "285: ' and'\n",
      "286: 'ou'\n",
      "287: ' m'\n",
      "288: ' d'\n",
      "289: 'ar'\n",
      "290: 'ion'\n",
      "291: 'ic'\n",
      "292: 'le'\n",
      "293: 'as'\n",
      "294: ' h'\n",
      "295: 'ro'\n",
      "296: ' th'\n",
      "297: 'ct'\n",
      "298: 'ent'\n",
      "299: ' re'\n",
      "300: 'il'\n",
      "301: ' e'\n",
      "302: 've'\n",
      "303: ' n'\n",
      "304: ' l'\n",
      "305: 'us'\n",
      "306: '.\n",
      "'\n",
      "307: 'om'\n",
      "308: 'ly'\n",
      "309: ' be'\n",
      "310: 'ation'\n",
      "311: ' is'\n",
      "312: 'im'\n",
      "313: ' g'\n",
      "314: ' T'\n",
      "315: 'ol'\n",
      "316: 'ra'\n",
      "317: 'ce'\n",
      "318: 'st'\n",
      "319: 'id'\n",
      "320: 'ur'\n",
      "321: 'ow'\n",
      "322: 'ot'\n",
      "323: 'ig'\n",
      "324: 'et'\n",
      "325: 'ut'\n",
      "326: '�'\n",
      "327: 'ch'\n",
      "328: ' for'\n",
      "329: ' that'\n",
      "330: ' A'\n",
      "331: ' on'\n",
      "332: 'ver'\n",
      "333: ' C'\n",
      "334: ' S'\n",
      "335: 'el'\n",
      "336: ' st'\n",
      "337: 'ir'\n",
      "338: 'ul'\n",
      "339: 'ay'\n",
      "340: ' I'\n",
      "341: ' as'\n",
      "342: ' pro'\n",
      "343: ' M'\n",
      "344: 'am'\n",
      "345: 'her'\n",
      "346: 'od'\n",
      "347: ' wh'\n",
      "348: 'se'\n",
      "349: ' ha'\n",
      "350: 'if'\n",
      "351: 'res'\n",
      "352: ' ('\n",
      "353: ' or'\n",
      "354: 'ith'\n",
      "355: ' y'\n",
      "356: ' an'\n",
      "357: ' con'\n",
      "358: 'em'\n",
      "359: ' with'\n",
      "360: 'ter'\n",
      "361: ' are'\n",
      "362: 'un'\n",
      "363: 'ect'\n",
      "364: ' it'\n",
      "365: ' P'\n",
      "366: ' al'\n",
      "367: ' The'\n",
      "368: 'ies'\n",
      "369: ' H'\n",
      "370: 'ure'\n",
      "371: 'ess'\n",
      "372: ' we'\n",
      "373: 'ge'\n",
      "374: 'op'\n",
      "375: ' de'\n",
      "376: ' he'\n",
      "377: 'ill'\n",
      "378: 'ad'\n",
      "379: 'ist'\n",
      "380: 'ate'\n",
      "381: ' B'\n",
      "382: 'igh'\n",
      "383: 'um'\n",
      "384: ' was'\n",
      "385: 'ant'\n",
      "386: ' at'\n",
      "387: 'ore'\n",
      "388: ' com'\n",
      "389: 'ity'\n",
      "390: 'ers'\n",
      "391: 'os'\n",
      "392: ' ex'\n",
      "393: ' you'\n",
      "394: 'th'\n",
      "395: 'iv'\n",
      "396: 'ain'\n",
      "397: ' G'\n",
      "398: ' by'\n",
      "399: 'pp'\n",
      "400: 'ive'\n",
      "401: 'rom'\n",
      "402: 'est'\n",
      "403: 'ment'\n",
      "404: 'ab'\n",
      "405: 'qu'\n",
      "406: 'ld'\n",
      "407: ' v'\n",
      "408: 'ort'\n",
      "409: 'and'\n",
      "410: 'ud'\n",
      "411: ' F'\n",
      "412: 'ke'\n",
      "413: ' E'\n",
      "414: ' us'\n",
      "415: ' su'\n",
      "416: ' not'\n",
      "417: 'ri'\n",
      "418: ' ch'\n",
      "419: ' from'\n",
      "420: ' R'\n",
      "421: ' D'\n",
      "422: ' W'\n",
      "423: ' r'\n",
      "424: ' L'\n",
      "425: 'oc'\n",
      "426: 'ac'\n",
      "427: 'nt'\n",
      "428: ' N'\n",
      "429: ' have'\n",
      "430: ' se'\n",
      "431: ' ne'\n",
      "432: ' le'\n",
      "433: ' sh'\n",
      "434: '00'\n",
      "435: 'pe'\n",
      "436: 'ress'\n",
      "437: '’'\n",
      "438: 'all'\n",
      "439: 'rou'\n",
      "440: ' this'\n",
      "441: 'ial'\n",
      "442: 'ther'\n",
      "443: 'ical'\n",
      "444: 'gh'\n",
      "445: 'ight'\n",
      "446: 'ich'\n",
      "447: 'red'\n",
      "448: 'pt'\n",
      "449: 'ated'\n",
      "450: 'du'\n",
      "451: ' can'\n",
      "452: 'ions'\n",
      "453: 'og'\n",
      "454: 'art'\n",
      "455: ' u'\n",
      "456: 'The'\n",
      "457: ' im'\n",
      "458: ' pl'\n",
      "459: ' which'\n",
      "460: ' wor'\n",
      "461: 'out'\n",
      "462: 'ould'\n",
      "463: 'ard'\n",
      "464: ' k'\n",
      "465: ' J'\n",
      "466: ''s'\n",
      "467: 'ff'\n",
      "468: 'ine'\n",
      "469: 'ome'\n",
      "470: ' ab'\n",
      "471: ' whe'\n",
      "472: 'ag'\n",
      "473: 'ust'\n",
      "474: 'act'\n",
      "475: 'per'\n",
      "476: 'ell'\n",
      "477: '19'\n",
      "478: 'ip'\n",
      "479: ' their'\n",
      "480: ' �'\n",
      "481: 'ear'\n",
      "482: ' \"'\n",
      "483: 'ans'\n",
      "484: 'ult'\n",
      "485: ' they'\n",
      "486: 'ost'\n",
      "487: ' en'\n",
      "488: ' int'\n",
      "489: 'ood'\n",
      "490: 'ide'\n",
      "491: 'iz'\n",
      "492: 'ast'\n",
      "493: 'cc'\n",
      "494: 'ak'\n",
      "495: ' O'\n",
      "496: 'reat'\n",
      "497: 'our'\n",
      "498: 'ew'\n",
      "499: ' but'\n",
      "500: ' cont'\n",
      "501: ' were'\n",
      "502: ' U'\n",
      "503: ' res'\n",
      "504: ' un'\n",
      "505: 'ance'\n",
      "506: 'ia'\n",
      "507: 'ous'\n",
      "508: ' will'\n",
      "509: 'ere'\n",
      "510: ' comp'\n",
      "511: ' all'\n",
      "512: 'ations'\n",
      "513: 'orm'\n",
      "514: ' do'\n",
      "515: ' inc'\n",
      "516: ' more'\n",
      "517: 'vel'\n",
      "518: 'ru'\n",
      "519: ' so'\n",
      "520: 'ong'\n",
      "521: ' dis'\n",
      "522: ' has'\n",
      "523: ' In'\n",
      "524: ' who'\n",
      "525: 'age'\n",
      "526: 'aus'\n",
      "527: 'are'\n",
      "528: ' per'\n",
      "529: '’s'\n",
      "530: ' his'\n",
      "531: ' ad'\n",
      "532: ' one'\n",
      "533: 'ild'\n",
      "534: ' man'\n",
      "535: 'port'\n",
      "536: 'av'\n",
      "537: 'so'\n",
      "538: 'ib'\n",
      "539: '”'\n",
      "540: 'ary'\n",
      "541: ' other'\n",
      "542: 'ase'\n",
      "543: 'ction'\n",
      "544: ' Th'\n",
      "545: 'ally'\n",
      "546: 'ry'\n",
      "547: ' press'\n",
      "548: 'ence'\n",
      "549: 'ount'\n",
      "550: 'able'\n",
      "551: 'ign'\n",
      "552: 'ap'\n",
      "553: ' app'\n",
      "554: ' them'\n",
      "555: ' tim'\n",
      "556: 'ame'\n",
      "557: 'ach'\n",
      "558: 'iff'\n",
      "559: 'ice'\n",
      "560: ' me'\n",
      "561: 'pl'\n",
      "562: 'now'\n",
      "563: '200'\n",
      "564: ' te'\n",
      "565: 'ition'\n",
      "566: 'ents'\n",
      "567: 'ok'\n",
      "568: ' pe'\n",
      "569: ' pre'\n",
      "570: ' j'\n",
      "571: ' bec'\n",
      "572: ' ar'\n",
      "573: 'rough'\n",
      "574: 'ord'\n",
      "575: 'ose'\n",
      "576: 'ens'\n",
      "577: ' cl'\n",
      "578: 'ake'\n",
      "579: 'ject'\n",
      "580: 'ire'\n",
      "581: ' -'\n",
      "582: 'ause'\n",
      "583: ' about'\n",
      "584: 'ound'\n",
      "585: 'te'\n",
      "586: ' pressure'\n",
      "587: ' K'\n",
      "588: '20'\n",
      "589: 'vers'\n",
      "590: ' ob'\n",
      "591: ' also'\n",
      "592: 'ater'\n",
      "593: ' there'\n",
      "594: 'ric'\n",
      "595: ' been'\n",
      "596: ' up'\n",
      "597: ' had'\n",
      "598: 'mer'\n",
      "599: ' out'\n",
      "600: 'ass'\n",
      "601: 'ber'\n",
      "602: ' stud'\n",
      "603: ' ev'\n",
      "604: 'hy'\n",
      "605: ' than'\n",
      "606: 'ang'\n",
      "607: ' “'\n",
      "608: 'ind'\n",
      "609: 'ail'\n",
      "610: ' diff'\n",
      "611: ' act'\n",
      "612: ' part'\n",
      "613: 'ace'\n",
      "614: 'ory'\n",
      "615: 'ub'\n",
      "616: ' tra'\n",
      "617: ' when'\n",
      "618: 'very'\n",
      "619: ' God'\n",
      "620: 'pec'\n",
      "621: 'ep'\n",
      "622: ' qu'\n",
      "623: 'yst'\n",
      "624: ' your'\n",
      "625: ' know'\n",
      "626: 'lect'\n",
      "627: ' sp'\n",
      "628: 'one'\n",
      "629: ' comm'\n",
      "630: 'ish'\n",
      "631: 'ie'\n",
      "632: 'form'\n",
      "633: 'ren'\n",
      "634: 'ile'\n",
      "635: 'ite'\n",
      "636: 'ffect'\n",
      "637: 'ian'\n",
      "638: 'erv'\n",
      "639: 'ople'\n",
      "640: 'ces'\n",
      "641: ' ag'\n",
      "642: ' rec'\n",
      "643: 'ities'\n",
      "644: 'ue'\n",
      "645: 'ks'\n",
      "646: 'low'\n",
      "647: ' sa'\n",
      "648: ' year'\n",
      "649: 'ays'\n",
      "650: 'olog'\n",
      "651: 'ike'\n",
      "652: ' may'\n",
      "653: ' high'\n",
      "654: 'ystem'\n",
      "655: ' some'\n",
      "656: ' des'\n",
      "657: ' He'\n",
      "658: 'nder'\n",
      "659: 'con'\n",
      "660: ' St'\n",
      "661: ' ac'\n",
      "662: ' time'\n",
      "663: ' go'\n",
      "664: ' people'\n",
      "665: ' It'\n",
      "666: 'tw'\n",
      "667: 'ree'\n",
      "668: ' its'\n",
      "669: 'oss'\n",
      "670: 'ple'\n",
      "671: 'ough'\n",
      "672: ' Ch'\n",
      "673: 'lic'\n",
      "674: 'velop'\n",
      "675: 'ob'\n",
      "676: ' over'\n",
      "677: ' if'\n",
      "678: 'cess'\n",
      "679: ' our'\n",
      "680: 'ors'\n",
      "681: 'wn'\n",
      "682: 'ings'\n",
      "683: 'ool'\n",
      "684: ' reg'\n",
      "685: 'ook'\n",
      "686: ' produ'\n",
      "687: 'ific'\n",
      "688: 'ove'\n",
      "689: ' dec'\n",
      "690: ' into'\n",
      "691: ').'\n",
      "692: ' ind'\n",
      "693: 'hed'\n",
      "694: 'ack'\n",
      "695: ' sc'\n",
      "696: 'int'\n",
      "697: ' through'\n",
      "698: 'ative'\n",
      "699: 'ced'\n",
      "700: '),'\n",
      "701: 'ied'\n",
      "702: 'tern'\n",
      "703: ' bl'\n",
      "704: 'fter'\n",
      "705: ' This'\n",
      "706: 'ph'\n",
      "707: 'ons'\n",
      "708: 'ature'\n",
      "709: ' these'\n",
      "710: ' cons'\n",
      "711: ' ra'\n",
      "712: ' would'\n",
      "713: 'ates'\n",
      "714: ' child'\n",
      "715: 'ased'\n",
      "716: 'ual'\n",
      "717: ' develop'\n",
      "718: 'ures'\n",
      "719: ' am'\n",
      "720: 'ram'\n",
      "721: ' only'\n",
      "722: 'ious'\n",
      "723: 'led'\n",
      "724: ' any'\n",
      "725: ' col'\n",
      "726: 'rib'\n",
      "727: ' Y'\n",
      "728: 'ds'\n",
      "729: ' ro'\n",
      "730: ' such'\n",
      "731: 'erm'\n",
      "732: ' spec'\n",
      "733: 'uring'\n",
      "734: 'ov'\n",
      "735: ' acc'\n",
      "736: ' effect'\n",
      "737: 'irst'\n",
      "738: '.\"'\n",
      "739: ' tr'\n",
      "740: ' imp'\n",
      "741: '201'\n",
      "742: 'hen'\n",
      "743: ' po'\n",
      "744: ' new'\n",
      "745: ' what'\n",
      "746: ' most'\n",
      "747: 'ning'\n",
      "748: ' differe'\n",
      "749: ' fe'\n",
      "750: ' incre'\n",
      "751: 'king'\n",
      "752: ' under'\n",
      "753: ' no'\n",
      "754: ' count'\n",
      "755: 'chool'\n",
      "756: ' like'\n",
      "757: ' tw'\n",
      "758: 'ient'\n",
      "759: 'ating'\n",
      "760: ' how'\n",
      "761: 'les'\n",
      "762: ' used'\n",
      "763: 'fore'\n",
      "764: 'ah'\n",
      "765: ' off'\n",
      "766: ' pat'\n",
      "767: ' pr'\n",
      "768: 'own'\n",
      "769: ' V'\n",
      "770: 'ange'\n",
      "771: 'io'\n",
      "772: ' use'\n",
      "773: 'rit'\n",
      "774: 'ise'\n",
      "775: ' need'\n",
      "776: ' because'\n",
      "777: 'lud'\n",
      "778: 'ane'\n",
      "779: ' gen'\n",
      "780: ' many'\n",
      "781: ' sur'\n",
      "782: 'ne'\n",
      "783: 'll'\n",
      "784: 'ful'\n",
      "785: 'ational'\n",
      "786: 'ract'\n",
      "787: 'ts'\n",
      "788: ' sub'\n",
      "789: ' system'\n",
      "790: 'iss'\n",
      "791: ' where'\n",
      "792: ' rem'\n",
      "793: 'ins'\n",
      "794: '199'\n",
      "795: 'ark'\n",
      "796: 'eth'\n",
      "797: ' hum'\n",
      "798: 'round'\n",
      "799: 'als'\n",
      "800: ' prov'\n",
      "801: ' co'\n",
      "802: 'ivers'\n",
      "803: ' def'\n",
      "804: ' first'\n",
      "805: 'hes'\n",
      "806: 'cy'\n",
      "807: 'ution'\n",
      "808: ' rel'\n",
      "809: ' her'\n",
      "810: 'elf'\n",
      "811: 'uch'\n",
      "812: 'old'\n",
      "813: ' hel'\n",
      "814: 'ants'\n",
      "815: 'uss'\n",
      "816: ' could'\n",
      "817: 'ility'\n",
      "818: ' Un'\n",
      "819: ' fl'\n",
      "820: ' two'\n",
      "821: 'ments'\n",
      "822: ' great'\n",
      "823: ' call'\n",
      "824: ' add'\n",
      "825: 'Th'\n",
      "826: ' help'\n",
      "827: ' him'\n",
      "828: 'een'\n",
      "829: 'ism'\n",
      "830: '.\n",
      "\n",
      "'\n",
      "831: ' includ'\n",
      "832: ' em'\n",
      "833: 'ular'\n",
      "834: ' mem'\n",
      "835: ' process'\n",
      "836: 'oth'\n",
      "837: ' tem'\n",
      "838: 'ural'\n",
      "839: 'ade'\n",
      "840: ' those'\n",
      "841: ' work'\n",
      "842: ' said'\n",
      "843: 'ower'\n",
      "844: ' activ'\n",
      "845: 'ting'\n",
      "846: ' ass'\n",
      "847: ' exp'\n",
      "848: ' form'\n",
      "849: 'tain'\n",
      "850: ' after'\n",
      "851: ' Re'\n",
      "852: 'hys'\n",
      "853: ' disc'\n",
      "854: ' mod'\n",
      "855: 'land'\n",
      "856: ' years'\n",
      "857: 'ten'\n",
      "858: 'row'\n",
      "859: ' As'\n",
      "860: 'ible'\n",
      "861: 'ng'\n",
      "862: 'ex'\n",
      "863: ' MP'\n",
      "864: 'ited'\n",
      "865: ' well'\n",
      "866: ' exam'\n",
      "867: 'ever'\n",
      "868: 'ick'\n",
      "869: 'rol'\n",
      "870: 'ince'\n",
      "871: ' MPa'\n",
      "872: ' inf'\n",
      "873: ' long'\n",
      "874: 'oy'\n",
      "875: ' betw'\n",
      "876: ' between'\n",
      "877: '10'\n",
      "878: ' show'\n",
      "879: ' import'\n",
      "880: ' way'\n",
      "881: ' How'\n",
      "882: ' should'\n",
      "883: ' min'\n",
      "884: 'gan'\n",
      "885: ' result'\n",
      "886: ' make'\n",
      "887: ' different'\n",
      "888: 'ork'\n",
      "889: ' supp'\n",
      "890: 'iversity'\n",
      "891: 'ually'\n",
      "892: ' pres'\n",
      "893: 'chn'\n",
      "894: ' num'\n",
      "895: 'ck'\n",
      "896: 'ink'\n",
      "897: 'vent'\n",
      "898: ' during'\n",
      "899: 'its'\n",
      "900: 'ology'\n",
      "901: 'ily'\n",
      "902: ' att'\n",
      "903: 'cept'\n",
      "904: 'meric'\n",
      "905: ' sign'\n",
      "906: ' bu'\n",
      "907: ' trans'\n",
      "908: ' treat'\n",
      "909: ')\n",
      "'\n",
      "910: ' right'\n",
      "911: 'ife'\n",
      "912: ' rep'\n",
      "913: ''t'\n",
      "914: 'ics'\n",
      "915: 'ft'\n",
      "916: ' For'\n",
      "917: ' sim'\n",
      "918: 'ata'\n",
      "919: ' found'\n",
      "920: 'ruct'\n",
      "921: 'alth'\n",
      "922: ' pop'\n",
      "923: ',\"'\n",
      "924: ' We'\n",
      "925: ' water'\n",
      "926: 'cent'\n",
      "927: ' just'\n",
      "928: ' get'\n",
      "929: ' ref'\n",
      "930: ' children'\n",
      "931: 'gram'\n",
      "932: ' techn'\n",
      "933: 'der'\n",
      "934: 'urn'\n",
      "935: 'ted'\n",
      "936: 'gg'\n",
      "937: ' resp'\n",
      "938: ' inter'\n",
      "939: ' sm'\n",
      "940: ' object'\n",
      "941: 'aking'\n",
      "942: 'ath'\n",
      "943: ' ke'\n",
      "944: 'formation'\n",
      "945: 'ize'\n",
      "946: 'In'\n",
      "947: 'me'\n",
      "948: 'to'\n",
      "949: '18'\n",
      "950: '||'\n",
      "951: ' det'\n",
      "952: ' same'\n",
      "953: 'arch'\n",
      "954: ' school'\n",
      "955: ' gra'\n",
      "956: ' ear'\n",
      "957: ' again'\n",
      "958: 'ness'\n",
      "959: 'iew'\n",
      "960: ' inst'\n",
      "961: 'roup'\n",
      "962: 'oci'\n",
      "963: 'ines'\n",
      "964: 'ives'\n",
      "965: 'other'\n",
      "966: ' mon'\n",
      "967: 'ense'\n",
      "968: ' pol'\n",
      "969: ' rese'\n",
      "970: ' Ar'\n",
      "971: 'hip'\n",
      "972: ' own'\n",
      "973: 'ident'\n",
      "974: ' desc'\n",
      "975: ' then'\n",
      "976: 'gy'\n",
      "977: 'ined'\n",
      "978: 'ared'\n",
      "979: ' Americ'\n",
      "980: ' gener'\n",
      "981: ' very'\n",
      "982: ' occ'\n",
      "983: 'ix'\n",
      "984: 'ining'\n",
      "985: ' she'\n",
      "986: 'irect'\n",
      "987: ' even'\n",
      "988: ' world'\n",
      "989: ' important'\n",
      "990: ' lar'\n",
      "991: ' art'\n",
      "992: ' does'\n",
      "993: 'aw'\n",
      "994: ':\n",
      "'\n",
      "995: ' redu'\n",
      "996: 'ars'\n",
      "997: '.,'\n",
      "998: 'ists'\n",
      "999: 'lex'\n",
      "1000: 'hysical'\n",
      "1001: ' sol'\n",
      "1002: ' before'\n",
      "1003: 'orn'\n",
      "1004: ' lear'\n",
      "1005: ' temper'\n",
      "1006: ' ma'\n",
      "1007: 'ized'\n",
      "1008: ' ext'\n",
      "1009: ' group'\n",
      "1010: ' elect'\n",
      "1011: ' pers'\n",
      "1012: 'hat'\n",
      "1013: 'ont'\n",
      "1014: ' sk'\n",
      "1015: ' Pro'\n",
      "1016: ' loc'\n",
      "1017: '..'\n",
      "1018: 'iron'\n",
      "1019: 'ave'\n",
      "1020: ' Fr'\n"
     ]
    }
   ],
   "source": [
    "for i in range(vocab_size-3):\n",
    "    print(f\"{i}: '{tokenizer.decode([i])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8967b6-f78b-4852-90f3-f3a1f37455cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
