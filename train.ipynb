{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb200416-d927-4302-b6b2-a0bfe159c321",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1ae964-36e4-4a3a-8109-306ce7cac387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that. \n",
    "# you prolly won't need this cell but running it won't hurt anything either\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426337be-2a22-41cb-8e46-494ed823c037",
   "metadata": {},
   "source": [
    "# Instantiate a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "209f309f-df73-4183-9d1c-a4fb834e825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelConfig(dim=32, device='cpu', out_weight_share=False, tokenizer='bpe_v2_tinyStories', vocab_len=512, num_layers=2, second_resid_norm=False, mlp_hidden_mult=4, mlp_bias=False, mlp_nonlinearity='SiLU', mlp_gated=True, num_q_heads=2, num_kv_heads=1, head_dim=16, theta=10000, max_seq_len=64, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06) \n",
      "\n",
      " TrainConfig(weight_decay=0.05, batch_size=32, max_iters=10, eval_interval=2, eval_samples=1, checkpoint_interval=None, lr_init=1e-06, lr_max=0.1, lr_min=0.001, warmup_iters=0, final_flat_iters=1, anneal_type='cos', num_restarts=3, T_mult=2)\n",
      "\n",
      "55.744K parameters\n",
      "\n",
      "Model(\n",
      "  (token_embedder): Embedding(515, 32)\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): MQA(\n",
      "        (Wq): Linear(in_features=32, out_features=32, bias=False)\n",
      "        (Wk): Linear(in_features=32, out_features=16, bias=False)\n",
      "        (Wv): Linear(in_features=32, out_features=16, bias=False)\n",
      "        (Wo): Linear(in_features=32, out_features=32, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wup): Linear(in_features=32, out_features=85, bias=False)\n",
      "        (Wgate): Linear(in_features=32, out_features=85, bias=False)\n",
      "        (Wdown): Linear(in_features=85, out_features=32, bias=False)\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (output): Linear(in_features=32, out_features=515, bias=False)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# config file\n",
    "from config import ModelConfig, TrainConfig\n",
    "cfg = ModelConfig()\n",
    "tcfg = TrainConfig()\n",
    "print(cfg, '\\n\\n', tcfg)\n",
    "\n",
    "# import the tokenizer specified by cfg\n",
    "from tools import import_from_nested_path\n",
    "imported_objects = import_from_nested_path(['tokenizers', cfg.tokenizer], 'tokenizer', ['get_tokenizer'])\n",
    "get_tokenizer = imported_objects.get('get_tokenizer')\n",
    "tokenizer = get_tokenizer(size = cfg.vocab_len)\n",
    "\n",
    "# model modules\n",
    "from modules.model import Model\n",
    "model = Model(cfg).to(cfg.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(f'\\n{sum(p.numel() for p in model.parameters())/1e3}K parameters\\n')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287e188-9a0d-47da-9d61-8885b29ba25d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6926710-6418-47c1-ba9f-ac9d63c092d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/tunadorable/.cache/huggingface/datasets/noanabeshima___json/noanabeshima--TinyStoriesV2-40971520ba3bacdf/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "Found cached dataset json (/Users/tunadorable/.cache/huggingface/datasets/noanabeshima___json/noanabeshima--TinyStoriesV2-40971520ba3bacdf/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tools import get_data_loader\n",
    "from train import scheduler_lambda, train\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = tcfg.lr_max, weight_decay = tcfg.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler_lambda)\n",
    "\n",
    "train_data_loader = get_data_loader(batch_size=tcfg.batch_size, split='train')\n",
    "test_data_loader = get_data_loader(batch_size=tcfg.batch_size, split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a85b7902-ee37-4963-b263-f28e72ecaf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # set to true if you'd like to see a graph of the learning rate schedule\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Generate learning rate values\n",
    "    lrs = [scheduler_lambda(i) for i in range(tcfg.max_iters)]\n",
    "    \n",
    "    # Plot the learning rates\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(lrs, label='Learning Rate')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32c471d7-d40a-48b9-ac3f-82371e85c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▊                                  | 1/10 [00:00<00:02,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0000: lr 0.010000, train loss 6.2944, val loss 6.3272, ppl 560, time elapsed: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████▍                          | 3/10 [00:00<00:01,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0002: lr 0.009831, train loss 6.0725, val loss 6.0611, ppl 429, time elapsed: 0.45 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████                   | 5/10 [00:00<00:00,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0004: lr 0.000269, train loss 5.9902, val loss 6.0047, ppl 405, time elapsed: 0.71 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████▌           | 7/10 [00:01<00:00,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0006: lr 0.006944, train loss 5.7022, val loss 5.6840, ppl 294, time elapsed: 0.98 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 10/10 [00:01<00:00,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0008: lr 0.001123, train loss 5.6418, val loss 5.5968, ppl 270, time elapsed: 1.24 seconds\n",
      "step 0009: lr 0.000100, train loss 5.6495, val loss 5.6145, ppl 274, time elapsed: 1.42 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, log_data = train(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    cfg, \n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    tcfg, \n",
    "    train_data_loader,\n",
    "    test_data_loader,\n",
    "    #log_data: list = None, \n",
    "    #detect_anomoly = False # use if you're getting crazy errors about a the gradient being broken\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc1adb-a1fd-4f0e-8666-26b2a99a0e54",
   "metadata": {},
   "source": [
    "# inference test before you decide to save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "565eeefb-ce04-44c5-a0b7-81a1a75786c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time time\n"
     ]
    }
   ],
   "source": [
    "from inference import generate\n",
    "prompt = \"Once upon a time\"\n",
    "model.eval()\n",
    "output = generate(\n",
    "    prompt, \n",
    "    model, \n",
    "    tokenizer,\n",
    "    #temperature = 0.9,\n",
    "    #top_k = 32,\n",
    "    #top_p = 0.9,\n",
    "    #max_gen_len = 512,\n",
    "    #memory_saver_div = 4,\n",
    ")\n",
    "model.train()\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf4ecb-0313-43a2-a35b-1f6c6e348427",
   "metadata": {},
   "source": [
    "# Saving your final model\n",
    "if `tcfg.checkpoint_interval != None` then checkpoints have already been saved\n",
    "\n",
    "you DO still need to do this even if you had been saving checkpoints; the final state has not yet been saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c31388-e8b5-48c6-b057-e185586b0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import save_model\n",
    "save_model(model, cfg, tcfg, log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303e3a1e-85cb-4fe6-a85b-1d3ec773eb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
