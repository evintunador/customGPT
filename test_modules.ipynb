{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea126987-59aa-4f76-b926-6d632887c30b",
   "metadata": {},
   "source": [
    "# This notebook is designed for teaching/testing purposes to help you visualize the tensor shapes that go through each module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f833b1f8-ea91-4ae5-b3a3-73e08e4c8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that. \n",
    "# you prolly won't need this cell but running it won't hurt anything either\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c2c04f-2dbd-4020-8d91-cc0e4e8511b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelConfig(dim=8, device='mps', linear_bias=False, out_weight_share=True, pos_enc_type='Sinusoidal', theta=10000, tokenizer='bpe_tinyStories', vocab_len=512, num_layers=2, second_resid_norm=False, mlp_hidden_mult=4, mlp_nonlinearity='Mish', mlp_gated=True, num_q_heads=2, num_kv_heads=1, head_dim=4, max_seq_len=10, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06)\n",
      "TrainConfig(model_name='2024-07-01|00-26-06', dataset_name='noanabeshima/TinyStoriesV2', data_subset=None, streaming=False, micro_batch_size=4, grad_accum_steps=2, max_iters=4, eval_interval=2, eval_samples=1, checkpoint_interval=None, beta1=0.9, beta2=0.95, epsilon=1e-08, weight_decay=0.05, grad_clip=1.0, lr_init=1e-06, lr_max=0.01, lr_min=0.0001, warmup_iters=0, final_flat_iters=0, anneal_type='cos', num_restarts=0, T_mult=2)\n"
     ]
    }
   ],
   "source": [
    "# config file\n",
    "from config import ModelConfig, TrainConfig\n",
    "cfg = ModelConfig()\n",
    "tcfg = TrainConfig()\n",
    "print(cfg)\n",
    "print(tcfg)\n",
    "\n",
    "# import the tokenizer specified by cfg\n",
    "from tools import import_from_nested_path\n",
    "imported_objects = import_from_nested_path(['custom_tokenizers', cfg.tokenizer], 'tokenizer', ['get_tokenizer'])\n",
    "get_tokenizer = imported_objects.get('get_tokenizer')\n",
    "tokenizer = get_tokenizer(size = 512) # assuming 'bpe', size options are 512, 1024 and 2048\n",
    "\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c12e3b-dc63-4479-ad55-b05d96364d1f",
   "metadata": {},
   "source": [
    "# Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3debc7b1-a7ec-4fb3-98cb-d16edf7c71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.norm import Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627969e9-9017-43f3-90ec-9a485abef26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.02K\n",
      "Norm()\n",
      "\n",
      "====================Entering Norm.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -3.219/2.720\n",
      "\n",
      "====================Entering Norm.RMSNorm====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -3.219/2.720\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.422/2.221\n",
      "====================Exiting Norm.RMSNorm====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.422/2.221\n",
      "====================Exiting Norm.forward====================\n",
      "CPU times: user 37.8 ms, sys: 54 ms, total: 91.8 ms\n",
      "Wall time: 141 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### RMSNorm\n",
    "\n",
    "# Create an instance of RMSNorm\n",
    "module = Norm(cfg.dim, 'RMSNorm').to(cfg.device)\n",
    "\n",
    "# let's take a look\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "\n",
    "# Initially, logging is disabled by default\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('CosineNorm')\n",
    "#module.disable_function_logging('LayerNorm')\n",
    "#module.disable_function_logging('RMSNorm')\n",
    "\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "output = module(x)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64020a96-205a-45b7-ae56-a3555d2b3719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================Entering Norm.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -3.307/3.083\n",
      "Full tensor content:\n",
      "tensor([[[-8.8062e-01,  5.9670e-01,  1.2619e+00,  5.5938e-01, -4.6171e-01,\n",
      "           3.6451e-01,  2.2062e+00, -1.6398e+00],\n",
      "         [-5.7208e-01,  3.4309e-01, -1.1433e+00,  1.9120e-01,  1.0663e+00,\n",
      "          -1.6467e+00, -7.2279e-01,  2.0599e-01],\n",
      "         [ 2.4912e-01, -2.2294e-01, -3.5382e-01, -7.9729e-01, -2.3183e+00,\n",
      "          -4.7812e-01,  5.4264e-01,  9.9761e-02],\n",
      "         [ 2.5057e+00, -6.3257e-01, -1.5550e+00,  8.8989e-01,  1.0273e+00,\n",
      "           5.5645e-01,  2.8017e-01, -1.4687e+00],\n",
      "         [ 3.0814e+00,  9.9468e-01, -1.7550e-01, -3.6665e-01,  4.2272e-01,\n",
      "           1.1503e+00, -7.4713e-02,  9.3969e-01],\n",
      "         [ 1.3014e+00, -1.5138e+00, -4.5624e-01, -8.4320e-01,  1.6013e+00,\n",
      "          -8.3619e-01,  2.5291e-01, -1.2946e+00],\n",
      "         [-4.9718e-01, -6.7348e-01, -3.1843e-01,  5.4570e-01,  1.0265e+00,\n",
      "           1.4287e+00, -1.3144e-01,  4.7876e-01],\n",
      "         [-8.0265e-01, -1.5666e+00, -1.1955e+00,  1.1205e+00, -2.0078e-01,\n",
      "           2.4315e-01,  2.2261e-01,  9.7725e-01],\n",
      "         [ 2.8413e-01,  1.2066e-01, -7.6657e-01,  1.5273e+00, -3.7489e-01,\n",
      "          -9.4992e-01, -1.4357e+00, -8.1794e-01],\n",
      "         [-1.3884e+00, -1.7996e+00,  1.3335e+00,  1.0422e+00, -1.2256e-01,\n",
      "           3.7046e-01, -1.0258e+00,  4.1417e-01]],\n",
      "\n",
      "        [[ 3.2520e-01,  9.4904e-01, -7.2985e-01, -6.0078e-01,  2.8819e-01,\n",
      "           3.8595e-01,  1.1451e+00,  2.0139e-01],\n",
      "         [-2.5347e+00,  2.3989e-01, -1.3864e+00,  1.1259e+00,  3.6031e-01,\n",
      "           4.4901e-01,  1.2517e-01, -5.7622e-01],\n",
      "         [ 1.1117e+00, -1.9521e-02, -1.6756e+00, -8.4518e-01, -1.5814e+00,\n",
      "           1.8569e-01,  1.5892e+00, -8.6376e-01],\n",
      "         [ 4.2200e-01, -9.1601e-01,  1.6627e+00, -1.1099e+00, -8.2095e-01,\n",
      "           3.0834e+00, -1.7652e+00, -7.1271e-01],\n",
      "         [ 9.0775e-01, -5.2705e-01, -1.0313e+00,  1.6864e-02, -2.5140e-01,\n",
      "           2.6186e+00, -9.1462e-01, -1.6701e+00],\n",
      "         [-1.8815e+00,  1.0706e+00, -1.9750e+00,  9.0723e-01,  9.7123e-02,\n",
      "          -1.5167e-03, -2.0391e-01, -1.6832e-01],\n",
      "         [ 1.0419e+00, -1.6354e+00,  7.7775e-01,  1.1437e+00,  3.2634e-01,\n",
      "          -1.7210e-01,  4.8884e-01, -8.2080e-02],\n",
      "         [-9.0006e-01,  9.6729e-01,  1.1941e-01, -1.4144e+00, -5.2260e-01,\n",
      "           2.1013e-01, -6.1607e-01, -1.5775e-02],\n",
      "         [-1.3561e+00,  3.2924e-01,  1.9036e-01,  1.8336e+00,  1.1477e+00,\n",
      "           2.0471e+00, -1.2423e+00, -3.3070e+00],\n",
      "         [-2.5826e-01,  1.3751e+00, -2.1588e+00, -1.6299e-01,  1.0001e+00,\n",
      "           4.0693e-01, -1.4246e+00,  9.8830e-01]],\n",
      "\n",
      "        [[ 2.3035e-02, -5.5052e-01, -6.7456e-01, -3.8581e-01, -1.3376e+00,\n",
      "          -4.0771e-01, -7.1910e-01,  2.3876e-01],\n",
      "         [-5.1576e-01, -1.9077e+00,  7.5490e-03, -5.7538e-01,  1.1914e+00,\n",
      "           4.8295e-02, -9.9740e-01, -5.3108e-01],\n",
      "         [ 6.1299e-01,  5.3537e-01, -4.1411e-01, -4.2935e-01, -5.5148e-02,\n",
      "           9.1080e-01,  1.0511e-01, -7.9107e-01],\n",
      "         [ 2.8930e+00,  1.1628e-01,  3.9675e-01, -1.3486e+00, -1.7246e-01,\n",
      "          -2.6317e-01, -1.0300e+00,  7.2263e-02],\n",
      "         [ 2.4131e-01, -1.1957e+00, -1.4056e+00,  2.0542e+00, -3.2025e-01,\n",
      "           1.4045e+00, -8.3209e-01, -5.1935e-01],\n",
      "         [-1.9734e+00,  2.4679e-02,  1.1677e+00,  7.9475e-01, -3.3332e-01,\n",
      "          -2.7711e-01,  1.2232e-01,  1.4873e+00],\n",
      "         [-1.2120e-01, -3.8039e-01, -1.3168e+00,  1.3951e+00, -4.9180e-01,\n",
      "          -1.1394e+00, -3.4132e-01,  1.8629e-01],\n",
      "         [ 3.9327e-01,  9.2481e-01, -1.2273e+00,  1.2329e-01, -2.5799e+00,\n",
      "           1.4686e-01,  1.0470e-01, -1.0486e+00],\n",
      "         [ 7.8738e-02,  2.8870e-01, -4.0981e-01,  3.3054e-01,  1.2984e+00,\n",
      "           8.4677e-01, -1.5178e+00, -1.7036e-02],\n",
      "         [ 7.2859e-01, -3.6092e-01, -2.5803e+00, -7.6885e-01,  8.6129e-02,\n",
      "          -8.4488e-02,  1.0265e+00, -5.5295e-01]],\n",
      "\n",
      "        [[-9.8240e-01, -2.5124e-01,  1.3513e+00, -9.0817e-01,  1.6886e+00,\n",
      "           5.7931e-01, -6.7827e-01, -5.1186e-01],\n",
      "         [ 6.1191e-01,  5.6149e-01,  1.3386e+00,  1.4186e+00, -1.8378e+00,\n",
      "          -8.3859e-01,  1.8320e+00,  1.1088e+00],\n",
      "         [ 2.7292e-01, -9.3049e-02,  3.7927e-01, -1.0269e+00, -2.6746e-01,\n",
      "           1.0894e+00,  1.3020e+00, -1.9888e-01],\n",
      "         [ 1.5984e+00,  1.4355e-01,  2.3144e-01,  7.4342e-01,  6.5687e-01,\n",
      "           1.9188e-01,  1.2319e+00,  4.3424e-01],\n",
      "         [ 7.1142e-01, -1.4441e+00,  1.3358e-01,  1.5802e+00, -8.0512e-01,\n",
      "          -4.0717e-01,  6.0294e-01, -2.1448e-01],\n",
      "         [ 1.3298e+00, -5.2588e-01, -2.0432e-01,  2.1151e+00,  5.3211e-01,\n",
      "           3.9458e-01,  8.8929e-01,  8.3381e-01],\n",
      "         [-5.3637e-01,  1.7090e+00,  3.3325e-01,  9.1676e-01,  1.0321e+00,\n",
      "          -1.2780e+00,  6.0153e-01,  3.8737e-01],\n",
      "         [-1.7911e+00, -1.0708e+00,  7.1495e-01,  1.5803e+00,  1.4097e+00,\n",
      "          -1.1268e+00, -7.2835e-01, -1.0844e-01],\n",
      "         [-2.1592e-01, -4.4334e-01,  4.2141e-01, -7.7901e-01, -6.8320e-01,\n",
      "          -1.2010e+00,  5.0395e-01, -1.0751e-01],\n",
      "         [-8.3023e-01,  2.5785e-01, -1.9191e+00,  1.0952e+00, -2.1749e-01,\n",
      "          -1.2206e+00, -4.2764e-03, -6.6637e-01]]], device='mps:0')\n",
      "\n",
      "====================Entering Norm.LayerNorm====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -3.307/3.083\n",
      "Full tensor content:\n",
      "tensor([[[-8.8062e-01,  5.9670e-01,  1.2619e+00,  5.5938e-01, -4.6171e-01,\n",
      "           3.6451e-01,  2.2062e+00, -1.6398e+00],\n",
      "         [-5.7208e-01,  3.4309e-01, -1.1433e+00,  1.9120e-01,  1.0663e+00,\n",
      "          -1.6467e+00, -7.2279e-01,  2.0599e-01],\n",
      "         [ 2.4912e-01, -2.2294e-01, -3.5382e-01, -7.9729e-01, -2.3183e+00,\n",
      "          -4.7812e-01,  5.4264e-01,  9.9761e-02],\n",
      "         [ 2.5057e+00, -6.3257e-01, -1.5550e+00,  8.8989e-01,  1.0273e+00,\n",
      "           5.5645e-01,  2.8017e-01, -1.4687e+00],\n",
      "         [ 3.0814e+00,  9.9468e-01, -1.7550e-01, -3.6665e-01,  4.2272e-01,\n",
      "           1.1503e+00, -7.4713e-02,  9.3969e-01],\n",
      "         [ 1.3014e+00, -1.5138e+00, -4.5624e-01, -8.4320e-01,  1.6013e+00,\n",
      "          -8.3619e-01,  2.5291e-01, -1.2946e+00],\n",
      "         [-4.9718e-01, -6.7348e-01, -3.1843e-01,  5.4570e-01,  1.0265e+00,\n",
      "           1.4287e+00, -1.3144e-01,  4.7876e-01],\n",
      "         [-8.0265e-01, -1.5666e+00, -1.1955e+00,  1.1205e+00, -2.0078e-01,\n",
      "           2.4315e-01,  2.2261e-01,  9.7725e-01],\n",
      "         [ 2.8413e-01,  1.2066e-01, -7.6657e-01,  1.5273e+00, -3.7489e-01,\n",
      "          -9.4992e-01, -1.4357e+00, -8.1794e-01],\n",
      "         [-1.3884e+00, -1.7996e+00,  1.3335e+00,  1.0422e+00, -1.2256e-01,\n",
      "           3.7046e-01, -1.0258e+00,  4.1417e-01]],\n",
      "\n",
      "        [[ 3.2520e-01,  9.4904e-01, -7.2985e-01, -6.0078e-01,  2.8819e-01,\n",
      "           3.8595e-01,  1.1451e+00,  2.0139e-01],\n",
      "         [-2.5347e+00,  2.3989e-01, -1.3864e+00,  1.1259e+00,  3.6031e-01,\n",
      "           4.4901e-01,  1.2517e-01, -5.7622e-01],\n",
      "         [ 1.1117e+00, -1.9521e-02, -1.6756e+00, -8.4518e-01, -1.5814e+00,\n",
      "           1.8569e-01,  1.5892e+00, -8.6376e-01],\n",
      "         [ 4.2200e-01, -9.1601e-01,  1.6627e+00, -1.1099e+00, -8.2095e-01,\n",
      "           3.0834e+00, -1.7652e+00, -7.1271e-01],\n",
      "         [ 9.0775e-01, -5.2705e-01, -1.0313e+00,  1.6864e-02, -2.5140e-01,\n",
      "           2.6186e+00, -9.1462e-01, -1.6701e+00],\n",
      "         [-1.8815e+00,  1.0706e+00, -1.9750e+00,  9.0723e-01,  9.7123e-02,\n",
      "          -1.5167e-03, -2.0391e-01, -1.6832e-01],\n",
      "         [ 1.0419e+00, -1.6354e+00,  7.7775e-01,  1.1437e+00,  3.2634e-01,\n",
      "          -1.7210e-01,  4.8884e-01, -8.2080e-02],\n",
      "         [-9.0006e-01,  9.6729e-01,  1.1941e-01, -1.4144e+00, -5.2260e-01,\n",
      "           2.1013e-01, -6.1607e-01, -1.5775e-02],\n",
      "         [-1.3561e+00,  3.2924e-01,  1.9036e-01,  1.8336e+00,  1.1477e+00,\n",
      "           2.0471e+00, -1.2423e+00, -3.3070e+00],\n",
      "         [-2.5826e-01,  1.3751e+00, -2.1588e+00, -1.6299e-01,  1.0001e+00,\n",
      "           4.0693e-01, -1.4246e+00,  9.8830e-01]],\n",
      "\n",
      "        [[ 2.3035e-02, -5.5052e-01, -6.7456e-01, -3.8581e-01, -1.3376e+00,\n",
      "          -4.0771e-01, -7.1910e-01,  2.3876e-01],\n",
      "         [-5.1576e-01, -1.9077e+00,  7.5490e-03, -5.7538e-01,  1.1914e+00,\n",
      "           4.8295e-02, -9.9740e-01, -5.3108e-01],\n",
      "         [ 6.1299e-01,  5.3537e-01, -4.1411e-01, -4.2935e-01, -5.5148e-02,\n",
      "           9.1080e-01,  1.0511e-01, -7.9107e-01],\n",
      "         [ 2.8930e+00,  1.1628e-01,  3.9675e-01, -1.3486e+00, -1.7246e-01,\n",
      "          -2.6317e-01, -1.0300e+00,  7.2263e-02],\n",
      "         [ 2.4131e-01, -1.1957e+00, -1.4056e+00,  2.0542e+00, -3.2025e-01,\n",
      "           1.4045e+00, -8.3209e-01, -5.1935e-01],\n",
      "         [-1.9734e+00,  2.4679e-02,  1.1677e+00,  7.9475e-01, -3.3332e-01,\n",
      "          -2.7711e-01,  1.2232e-01,  1.4873e+00],\n",
      "         [-1.2120e-01, -3.8039e-01, -1.3168e+00,  1.3951e+00, -4.9180e-01,\n",
      "          -1.1394e+00, -3.4132e-01,  1.8629e-01],\n",
      "         [ 3.9327e-01,  9.2481e-01, -1.2273e+00,  1.2329e-01, -2.5799e+00,\n",
      "           1.4686e-01,  1.0470e-01, -1.0486e+00],\n",
      "         [ 7.8738e-02,  2.8870e-01, -4.0981e-01,  3.3054e-01,  1.2984e+00,\n",
      "           8.4677e-01, -1.5178e+00, -1.7036e-02],\n",
      "         [ 7.2859e-01, -3.6092e-01, -2.5803e+00, -7.6885e-01,  8.6129e-02,\n",
      "          -8.4488e-02,  1.0265e+00, -5.5295e-01]],\n",
      "\n",
      "        [[-9.8240e-01, -2.5124e-01,  1.3513e+00, -9.0817e-01,  1.6886e+00,\n",
      "           5.7931e-01, -6.7827e-01, -5.1186e-01],\n",
      "         [ 6.1191e-01,  5.6149e-01,  1.3386e+00,  1.4186e+00, -1.8378e+00,\n",
      "          -8.3859e-01,  1.8320e+00,  1.1088e+00],\n",
      "         [ 2.7292e-01, -9.3049e-02,  3.7927e-01, -1.0269e+00, -2.6746e-01,\n",
      "           1.0894e+00,  1.3020e+00, -1.9888e-01],\n",
      "         [ 1.5984e+00,  1.4355e-01,  2.3144e-01,  7.4342e-01,  6.5687e-01,\n",
      "           1.9188e-01,  1.2319e+00,  4.3424e-01],\n",
      "         [ 7.1142e-01, -1.4441e+00,  1.3358e-01,  1.5802e+00, -8.0512e-01,\n",
      "          -4.0717e-01,  6.0294e-01, -2.1448e-01],\n",
      "         [ 1.3298e+00, -5.2588e-01, -2.0432e-01,  2.1151e+00,  5.3211e-01,\n",
      "           3.9458e-01,  8.8929e-01,  8.3381e-01],\n",
      "         [-5.3637e-01,  1.7090e+00,  3.3325e-01,  9.1676e-01,  1.0321e+00,\n",
      "          -1.2780e+00,  6.0153e-01,  3.8737e-01],\n",
      "         [-1.7911e+00, -1.0708e+00,  7.1495e-01,  1.5803e+00,  1.4097e+00,\n",
      "          -1.1268e+00, -7.2835e-01, -1.0844e-01],\n",
      "         [-2.1592e-01, -4.4334e-01,  4.2141e-01, -7.7901e-01, -6.8320e-01,\n",
      "          -1.2010e+00,  5.0395e-01, -1.0751e-01],\n",
      "         [-8.3023e-01,  2.5785e-01, -1.9191e+00,  1.0952e+00, -2.1749e-01,\n",
      "          -1.2206e+00, -4.2764e-03, -6.6637e-01]]], device='mps:0')\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.315/2.347\n",
      "Full tensor content:\n",
      "tensor([[[-0.9906,  0.3028,  0.8853,  0.2702, -0.6238,  0.0995,  1.7120,\n",
      "          -1.6553],\n",
      "         [-0.3448,  0.7536, -1.0304,  0.5713,  1.6216, -1.6346, -0.5257,\n",
      "           0.5890],\n",
      "         [ 0.7994,  0.2268,  0.0680, -0.4700, -2.3151, -0.0828,  1.1555,\n",
      "           0.6182],\n",
      "         [ 1.7968, -0.6493, -1.3683,  0.5374,  0.6445,  0.2775,  0.0622,\n",
      "          -1.3010],\n",
      "         [ 2.2543,  0.2396, -0.8902, -1.0747, -0.3126,  0.3898, -0.7929,\n",
      "           0.1865],\n",
      "         [ 1.3988, -1.1835, -0.2134, -0.5684,  1.6739, -0.5620,  0.4370,\n",
      "          -0.9825],\n",
      "         [-1.0306, -1.2796, -0.7781,  0.4426,  1.1218,  1.6899, -0.5139,\n",
      "           0.3480],\n",
      "         [-0.7126, -1.5469, -1.1416,  1.3879, -0.0552,  0.4297,  0.4072,\n",
      "           1.2314],\n",
      "         [ 0.6730,  0.4852, -0.5342,  2.1012, -0.0842, -0.7448, -1.3030,\n",
      "          -0.5932],\n",
      "         [-1.1555, -1.5382,  1.3781,  1.1069,  0.0227,  0.4816, -0.8180,\n",
      "           0.5223]],\n",
      "\n",
      "        [[ 0.1301,  1.1485, -1.5924, -1.3817,  0.0696,  0.2292,  1.4686,\n",
      "          -0.0721],\n",
      "         [-2.0475,  0.4661, -1.0072,  1.2688,  0.5752,  0.6556,  0.3622,\n",
      "          -0.2732],\n",
      "         [ 1.2281,  0.2170, -1.2631, -0.5209, -1.1789,  0.4004,  1.6549,\n",
      "          -0.5375],\n",
      "         [ 0.2879, -0.5845,  1.0969, -0.7110, -0.5225,  2.0233, -1.1382,\n",
      "          -0.4520],\n",
      "         [ 0.8074, -0.3349, -0.7363,  0.0981, -0.1154,  2.1695, -0.6435,\n",
      "          -1.2450],\n",
      "         [-1.5277,  1.2699, -1.6163,  1.1150,  0.3473,  0.2539,  0.0621,\n",
      "           0.0958],\n",
      "         [ 0.9623, -2.2351,  0.6469,  1.0839,  0.1078, -0.4875,  0.3018,\n",
      "          -0.3800],\n",
      "         [-0.9040,  1.7816,  0.5622, -1.6437, -0.3611,  0.6927, -0.4955,\n",
      "           0.3678],\n",
      "         [-0.7682,  0.2190,  0.1377,  1.1002,  0.6984,  1.2253, -0.7015,\n",
      "          -1.9109],\n",
      "         [-0.1970,  1.2081, -1.8320, -0.1150,  0.8855,  0.3753, -1.2004,\n",
      "           0.8754]],\n",
      "\n",
      "        [[ 1.1108, -0.1641, -0.4398,  0.2020, -1.9136,  0.1533, -0.5388,\n",
      "           1.5903],\n",
      "         [-0.1265, -1.7916,  0.4995, -0.1978,  1.9157,  0.5483, -0.7027,\n",
      "          -0.1448],\n",
      "         [ 0.9986,  0.8586, -0.8539, -0.8813, -0.2065,  1.5357,  0.0826,\n",
      "          -1.5337],\n",
      "         [ 2.3468,  0.0278,  0.2620, -1.1957, -0.2134, -0.2891, -0.9295,\n",
      "          -0.0090],\n",
      "         [ 0.2708, -0.9729, -1.1545,  1.8399, -0.2152,  1.2776, -0.6582,\n",
      "          -0.3875],\n",
      "         [-2.0772, -0.1008,  1.0297,  0.6609, -0.4549, -0.3993, -0.0042,\n",
      "           1.3459],\n",
      "         [ 0.1976, -0.1329, -1.3269,  2.1310, -0.2749, -1.1006, -0.0831,\n",
      "           0.5897],\n",
      "         [ 0.7407,  1.2399, -0.7814,  0.4871, -2.0518,  0.5093,  0.4697,\n",
      "          -0.6135],\n",
      "         [-0.0426,  0.2237, -0.6622,  0.2768,  1.5044,  0.9315, -2.0676,\n",
      "          -0.1641],\n",
      "         [ 1.0108, -0.0462, -2.1995, -0.4420,  0.3875,  0.2220,  1.2998,\n",
      "          -0.2325]],\n",
      "\n",
      "        [[-1.0463, -0.2950,  1.3515, -0.9700,  1.6981,  0.5583, -0.7338,\n",
      "          -0.5628],\n",
      "         [ 0.0748,  0.0317,  0.6956,  0.7639, -2.0181, -1.1644,  1.1172,\n",
      "           0.4993],\n",
      "         [ 0.1282, -0.3888,  0.2785, -1.7080, -0.6351,  1.2815,  1.5819,\n",
      "          -0.5382],\n",
      "         [ 1.9204, -1.0379, -0.8591,  0.1819,  0.0059, -0.9396,  1.1752,\n",
      "          -0.4468],\n",
      "         [ 0.7779, -1.6460,  0.1281,  1.7548, -0.9274, -0.4800,  0.6559,\n",
      "          -0.2633],\n",
      "         [ 0.8433, -1.5306, -1.1192,  1.8481, -0.1771, -0.3531,  0.2798,\n",
      "           0.2088],\n",
      "         [-1.0659,  1.5018, -0.0714,  0.5958,  0.7278, -1.9139,  0.2354,\n",
      "          -0.0095],\n",
      "         [-1.4077, -0.7935,  0.7290,  1.4668,  1.3213, -0.8413, -0.5016,\n",
      "           0.0270],\n",
      "         [ 0.1767, -0.2369,  1.3359, -0.8475, -0.6732, -1.6151,  1.4860,\n",
      "           0.3739],\n",
      "         [-0.4496,  0.7981, -1.6983,  1.7583,  0.2530, -0.8973,  0.4975,\n",
      "          -0.2617]]], device='mps:0')\n",
      "====================Exiting Norm.LayerNorm====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.315/2.347\n",
      "Full tensor content:\n",
      "tensor([[[-0.9906,  0.3028,  0.8853,  0.2702, -0.6238,  0.0995,  1.7120,\n",
      "          -1.6553],\n",
      "         [-0.3448,  0.7536, -1.0304,  0.5713,  1.6216, -1.6346, -0.5257,\n",
      "           0.5890],\n",
      "         [ 0.7994,  0.2268,  0.0680, -0.4700, -2.3151, -0.0828,  1.1555,\n",
      "           0.6182],\n",
      "         [ 1.7968, -0.6493, -1.3683,  0.5374,  0.6445,  0.2775,  0.0622,\n",
      "          -1.3010],\n",
      "         [ 2.2543,  0.2396, -0.8902, -1.0747, -0.3126,  0.3898, -0.7929,\n",
      "           0.1865],\n",
      "         [ 1.3988, -1.1835, -0.2134, -0.5684,  1.6739, -0.5620,  0.4370,\n",
      "          -0.9825],\n",
      "         [-1.0306, -1.2796, -0.7781,  0.4426,  1.1218,  1.6899, -0.5139,\n",
      "           0.3480],\n",
      "         [-0.7126, -1.5469, -1.1416,  1.3879, -0.0552,  0.4297,  0.4072,\n",
      "           1.2314],\n",
      "         [ 0.6730,  0.4852, -0.5342,  2.1012, -0.0842, -0.7448, -1.3030,\n",
      "          -0.5932],\n",
      "         [-1.1555, -1.5382,  1.3781,  1.1069,  0.0227,  0.4816, -0.8180,\n",
      "           0.5223]],\n",
      "\n",
      "        [[ 0.1301,  1.1485, -1.5924, -1.3817,  0.0696,  0.2292,  1.4686,\n",
      "          -0.0721],\n",
      "         [-2.0475,  0.4661, -1.0072,  1.2688,  0.5752,  0.6556,  0.3622,\n",
      "          -0.2732],\n",
      "         [ 1.2281,  0.2170, -1.2631, -0.5209, -1.1789,  0.4004,  1.6549,\n",
      "          -0.5375],\n",
      "         [ 0.2879, -0.5845,  1.0969, -0.7110, -0.5225,  2.0233, -1.1382,\n",
      "          -0.4520],\n",
      "         [ 0.8074, -0.3349, -0.7363,  0.0981, -0.1154,  2.1695, -0.6435,\n",
      "          -1.2450],\n",
      "         [-1.5277,  1.2699, -1.6163,  1.1150,  0.3473,  0.2539,  0.0621,\n",
      "           0.0958],\n",
      "         [ 0.9623, -2.2351,  0.6469,  1.0839,  0.1078, -0.4875,  0.3018,\n",
      "          -0.3800],\n",
      "         [-0.9040,  1.7816,  0.5622, -1.6437, -0.3611,  0.6927, -0.4955,\n",
      "           0.3678],\n",
      "         [-0.7682,  0.2190,  0.1377,  1.1002,  0.6984,  1.2253, -0.7015,\n",
      "          -1.9109],\n",
      "         [-0.1970,  1.2081, -1.8320, -0.1150,  0.8855,  0.3753, -1.2004,\n",
      "           0.8754]],\n",
      "\n",
      "        [[ 1.1108, -0.1641, -0.4398,  0.2020, -1.9136,  0.1533, -0.5388,\n",
      "           1.5903],\n",
      "         [-0.1265, -1.7916,  0.4995, -0.1978,  1.9157,  0.5483, -0.7027,\n",
      "          -0.1448],\n",
      "         [ 0.9986,  0.8586, -0.8539, -0.8813, -0.2065,  1.5357,  0.0826,\n",
      "          -1.5337],\n",
      "         [ 2.3468,  0.0278,  0.2620, -1.1957, -0.2134, -0.2891, -0.9295,\n",
      "          -0.0090],\n",
      "         [ 0.2708, -0.9729, -1.1545,  1.8399, -0.2152,  1.2776, -0.6582,\n",
      "          -0.3875],\n",
      "         [-2.0772, -0.1008,  1.0297,  0.6609, -0.4549, -0.3993, -0.0042,\n",
      "           1.3459],\n",
      "         [ 0.1976, -0.1329, -1.3269,  2.1310, -0.2749, -1.1006, -0.0831,\n",
      "           0.5897],\n",
      "         [ 0.7407,  1.2399, -0.7814,  0.4871, -2.0518,  0.5093,  0.4697,\n",
      "          -0.6135],\n",
      "         [-0.0426,  0.2237, -0.6622,  0.2768,  1.5044,  0.9315, -2.0676,\n",
      "          -0.1641],\n",
      "         [ 1.0108, -0.0462, -2.1995, -0.4420,  0.3875,  0.2220,  1.2998,\n",
      "          -0.2325]],\n",
      "\n",
      "        [[-1.0463, -0.2950,  1.3515, -0.9700,  1.6981,  0.5583, -0.7338,\n",
      "          -0.5628],\n",
      "         [ 0.0748,  0.0317,  0.6956,  0.7639, -2.0181, -1.1644,  1.1172,\n",
      "           0.4993],\n",
      "         [ 0.1282, -0.3888,  0.2785, -1.7080, -0.6351,  1.2815,  1.5819,\n",
      "          -0.5382],\n",
      "         [ 1.9204, -1.0379, -0.8591,  0.1819,  0.0059, -0.9396,  1.1752,\n",
      "          -0.4468],\n",
      "         [ 0.7779, -1.6460,  0.1281,  1.7548, -0.9274, -0.4800,  0.6559,\n",
      "          -0.2633],\n",
      "         [ 0.8433, -1.5306, -1.1192,  1.8481, -0.1771, -0.3531,  0.2798,\n",
      "           0.2088],\n",
      "         [-1.0659,  1.5018, -0.0714,  0.5958,  0.7278, -1.9139,  0.2354,\n",
      "          -0.0095],\n",
      "         [-1.4077, -0.7935,  0.7290,  1.4668,  1.3213, -0.8413, -0.5016,\n",
      "           0.0270],\n",
      "         [ 0.1767, -0.2369,  1.3359, -0.8475, -0.6732, -1.6151,  1.4860,\n",
      "           0.3739],\n",
      "         [-0.4496,  0.7981, -1.6983,  1.7583,  0.2530, -0.8973,  0.4975,\n",
      "          -0.2617]]], device='mps:0', grad_fn=<AddBackward0>)\n",
      "====================Exiting Norm.forward====================\n",
      "CPU times: user 77.3 ms, sys: 64.3 ms, total: 142 ms\n",
      "Wall time: 298 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# LayerNorm\n",
    "module = Norm(cfg.dim, 'LayerNorm').to(cfg.device)\n",
    "module.enable_logging()\n",
    "\n",
    "# you can also have it optionally print out all tensors in full\n",
    "module.enable_full_tensor_printing()\n",
    "# i recommend only doing this with very small toy values for your hyperparameters, otherwise this gets too big\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('CosineNorm')\n",
    "#module.disable_function_logging('LayerNorm')\n",
    "#module.disable_function_logging('RMSNorm')\n",
    "\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "output = module(x)\n",
    "module.disable_logging()\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e190f3de-37fd-442b-bfb1-6a090115fc75",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a89e1e8f-cedd-4885-ad50-934827ed045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.attention import SelfAttention, PrecomputeRotaryFrequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e8f27b4-6d1f-4fcd-99b0-2284e65d6b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.19K\n",
      "SelfAttention(\n",
      "  (Wq): Linear(in_features=8, out_features=8, bias=False)\n",
      "  (Wk): Linear(in_features=8, out_features=4, bias=False)\n",
      "  (Wv): Linear(in_features=8, out_features=4, bias=False)\n",
      "  (Wo): Linear(in_features=8, out_features=8, bias=False)\n",
      ")\n",
      "\n",
      "====================Entering SelfAttention.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.463/2.436\n",
      "Other-type 'freqs': Type=NoneType, Value=None\n",
      "Tensor 'mask' shape: torch.Size([10, 10]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering SelfAttention.match_headcount====================\n",
      "Inputs:\n",
      "Tensor 'k' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.575/1.578\n",
      "Tensor 'v' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.064/1.400\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 10, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.575/1.578\n",
      "Tensor 'output[1]' shape: torch.Size([4, 10, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.064/1.400\n",
      "====================Exiting SelfAttention.match_headcount====================\n",
      "\n",
      "====================Entering SelfAttention.flash_attention====================\n",
      "Inputs:\n",
      "Tensor 'q' shape: torch.Size([4, 2, 10, 4]), dtype: torch.float32, device: mps:0, min/max: -1.736/1.712\n",
      "Tensor 'k' shape: torch.Size([4, 2, 10, 4]), dtype: torch.float32, device: mps:0, min/max: -1.575/1.578\n",
      "Tensor 'v' shape: torch.Size([4, 2, 10, 4]), dtype: torch.float32, device: mps:0, min/max: -1.064/1.400\n",
      "Tensor 'mask' shape: torch.Size([10, 10]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 2, 10, 4]), dtype: torch.float32, device: mps:0, min/max: -1.076/1.018\n",
      "====================Exiting SelfAttention.flash_attention====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.565/0.716\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting SelfAttention.forward====================\n",
      "CPU times: user 74.4 ms, sys: 13.9 ms, total: 88.3 ms\n",
      "Wall time: 120 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# first up let's look at training\n",
    "\n",
    "# Create an instance of multi-head self-attention\n",
    "module = SelfAttention(cfg.dim, cfg.head_dim, cfg.num_q_heads, cfg.num_kv_heads, cfg.max_seq_len, cfg.linear_bias, device=cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "\n",
    "# Initially, logging is disabled by default\n",
    "module.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('apply_precompute_freqs')\n",
    "#module.disable_function_logging('reshape_for_broadcast')\n",
    "#module.disable_function_logging('match_headcount')\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('calc_output')\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "if cfg.pos_enc_type == 'RoPE':\n",
    "    precompute_freqs = PrecomputeRotaryFrequencies(cfg.head_dim, cfg.max_seq_len, cfg.theta, cfg.device)\n",
    "    freqs = precompute_freqs()\n",
    "else:\n",
    "    freqs = None\n",
    "mask = torch.ones(cfg.max_seq_len, cfg.max_seq_len, dtype=torch.bool, device=cfg.device).triu(diagonal=1)\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "output = module(x, freqs, mask, training=True)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, freqs, mask, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7e034c-c5a6-43ac-b6d2-bbb71f9965ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================Entering SelfAttention.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 2, 8]), dtype: torch.float32, device: mps:0, min/max: -1.952/2.330\n",
      "Other-type 'freqs': Type=NoneType, Value=None\n",
      "Tensor 'mask' shape: torch.Size([2, 6]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Integer 'cache_len': Value=4\n",
      "Dict 'kv_cache':\n",
      "    Tensor 'kv_cache[k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Tensor 'kv_cache[v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "Bool 'training': Value=False\n",
      "\n",
      "====================Entering SelfAttention.match_headcount====================\n",
      "Inputs:\n",
      "Tensor 'k' shape: torch.Size([4, 6, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.855/0.928\n",
      "Tensor 'v' shape: torch.Size([4, 6, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.279/1.733\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 6, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -0.855/0.928\n",
      "Tensor 'output[1]' shape: torch.Size([4, 6, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.279/1.733\n",
      "====================Exiting SelfAttention.match_headcount====================\n",
      "\n",
      "====================Entering SelfAttention.flash_attention====================\n",
      "Inputs:\n",
      "Tensor 'q' shape: torch.Size([4, 2, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.013/0.998\n",
      "Tensor 'k' shape: torch.Size([4, 2, 6, 4]), dtype: torch.float32, device: mps:0, min/max: -0.855/0.928\n",
      "Tensor 'v' shape: torch.Size([4, 2, 6, 4]), dtype: torch.float32, device: mps:0, min/max: -1.279/1.733\n",
      "Tensor 'mask' shape: torch.Size([2, 6]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Bool 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 2, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -0.240/0.325\n",
      "====================Exiting SelfAttention.flash_attention====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 2, 8]), dtype: torch.float32, device: mps:0, min/max: -0.118/0.167\n",
      "Dict 'output[1]':\n",
      "    Tensor 'output[1][k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.855/0.928\n",
      "    Tensor 'output[1][v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.279/1.733\n",
      "====================Exiting SelfAttention.forward====================\n",
      "CPU times: user 49.2 ms, sys: 7.27 ms, total: 56.5 ms\n",
      "Wall time: 67.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# now let's do it for inference\n",
    "\n",
    "module = SelfAttention(cfg.dim, cfg.head_dim, cfg.num_q_heads, cfg.num_kv_heads, cfg.max_seq_len, cfg.linear_bias, device=cfg.device)\n",
    "module.enable_logging()\n",
    "#module.disable_function_logging('apply_precompute_freqs')\n",
    "#module.disable_function_logging('reshape_for_broadcast')\n",
    "#module.disable_function_logging('match_headcount')\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('calc_output')\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "if cfg.pos_enc_type == 'RoPE':\n",
    "    precompute_freqs = PrecomputeRotaryFrequencies(cfg.head_dim, cfg.max_seq_len, cfg.theta, cfg.device)\n",
    "    freqs = precompute_freqs()\n",
    "else:\n",
    "    freqs = None\n",
    "mask = torch.ones(cfg.max_seq_len, cfg.max_seq_len, dtype=torch.bool, device=cfg.device).triu(diagonal=1)\n",
    "# setting up for kv caching\n",
    "context_chunk_len = cfg.max_seq_len // 4\n",
    "cache_len = random.randint(1, 3 * context_chunk_len)\n",
    "seq_len = cache_len + context_chunk_len\n",
    "kv_cache = {\n",
    "    'k': torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device),\n",
    "    'v': torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device)\n",
    "}\n",
    "# need to extend the mask with zeros for the cached values\n",
    "mask = torch.nn.functional.pad(mask[:context_chunk_len, :context_chunk_len], (cache_len, 0, 0, 0), value=False).bool()\n",
    "x = torch.randn(tcfg.micro_batch_size,context_chunk_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "output = module(x, freqs, mask, cache_len, kv_cache)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, freqs, mask, cache_len, context_chunk_len, seq_len, kv_cache, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb308c5-b578-46f2-86ae-bfa6800be641",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7736b685-f941-4182-a5b7-4731cce706b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.mlp import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c62e49e9-2189-4269-968e-1df99469dc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.50K\n",
      "MLP(\n",
      "  (Wup): Linear(in_features=8, out_features=21, bias=False)\n",
      "  (Wgate): Linear(in_features=8, out_features=21, bias=False)\n",
      "  (Wdown): Linear(in_features=21, out_features=8, bias=False)\n",
      "  (nonlinearity): GELU(approximate='none')\n",
      ")\n",
      "\n",
      "====================Entering MLP.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.760/3.321\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.787/0.672\n",
      "====================Exiting MLP.forward====================\n",
      "CPU times: user 15.8 ms, sys: 2.39 ms, total: 18.1 ms\n",
      "Wall time: 21.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# GeGLU\n",
    "module = MLP(\n",
    "    cfg.dim, \n",
    "    int(cfg.dim * cfg.mlp_hidden_mult * 2/3), \n",
    "    cfg.dim, \n",
    "    'GeLU', \n",
    "    gated=True, \n",
    "    bias=cfg.linear_bias, \n",
    "    dropout_rate = 0.1\n",
    ").to(cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "module.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "output = module(x, training=True)\n",
    "module.disable_logging()\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0748fa3-3230-4dd7-a768-1256ea72e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.55K\n",
      "MLP(\n",
      "  (Wup): Linear(in_features=8, out_features=32, bias=True)\n",
      "  (Wdown): Linear(in_features=32, out_features=8, bias=True)\n",
      "  (nonlinearity): ReLU()\n",
      ")\n",
      "\n",
      "====================Entering MLP.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.375/2.594\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -1.070/0.788\n",
      "====================Exiting MLP.forward====================\n",
      "CPU times: user 17.4 ms, sys: 3.04 ms, total: 20.5 ms\n",
      "Wall time: 24.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# not gated, testing every other nonlinearity\n",
    "module = MLP(\n",
    "    cfg.dim, \n",
    "    cfg.dim * cfg.mlp_hidden_mult, \n",
    "    cfg.dim, \n",
    "    'ReLU', \n",
    "    gated=False, \n",
    "    bias=True, \n",
    "    dropout_rate = 0.1\n",
    ").to(cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "module.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "output = module(x, training=True)\n",
    "module.disable_logging()\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3a502f-4646-4a02-9412-372482af9fa0",
   "metadata": {},
   "source": [
    "# ResidualLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a21d708-af47-4f32-b111-08efedc584f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.layer import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c886661d-5a26-4787-93d1-c3c8c6b1c02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.73K\n",
      "Layer(\n",
      "  (pre_attn_norm): Norm()\n",
      "  (attn): SelfAttention(\n",
      "    (Wq): Linear(in_features=8, out_features=8, bias=False)\n",
      "    (Wk): Linear(in_features=8, out_features=4, bias=False)\n",
      "    (Wv): Linear(in_features=8, out_features=4, bias=False)\n",
      "    (Wo): Linear(in_features=8, out_features=8, bias=False)\n",
      "  )\n",
      "  (pre_mlp_norm): Norm()\n",
      "  (mlp): MLP(\n",
      "    (Wup): Linear(in_features=8, out_features=21, bias=False)\n",
      "    (Wgate): Linear(in_features=8, out_features=21, bias=False)\n",
      "    (Wdown): Linear(in_features=21, out_features=8, bias=False)\n",
      "    (nonlinearity): Mish()\n",
      "  )\n",
      ")\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -3.690/2.812\n",
      "Other-type 'freqs': Type=NoneType, Value=None\n",
      "Tensor 'mask' shape: torch.Size([10, 10]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -3.690/2.812\n",
      "Other-type 'freqs': Type=NoneType, Value=None\n",
      "Tensor 'mask' shape: torch.Size([10, 10]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.770/0.704\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -3.644/3.005\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.622/0.708\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -3.607/2.896\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.forward====================\n",
      "CPU times: user 18.5 ms, sys: 3.28 ms, total: 21.8 ms\n",
      "Wall time: 27.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TRAINING\n",
    "module = Layer(cfg).to(cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "#module.disable_function_logging('attn_connect')\n",
    "#module.disable_function_logging('mlp_connect')\n",
    "### enabling printing for sub-modules\n",
    "#module.pre_attn_norm.enable_logging()\n",
    "#module.attn.enable_logging()\n",
    "#module.post_attn_norm.enable_logging()\n",
    "#module.pre_mlp_norm.enable_logging()\n",
    "#module.mlp.enable_logging()\n",
    "#module.post_mlp_norm.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "if cfg.pos_enc_type == 'RoPE':\n",
    "    precompute_freqs = PrecomputeRotaryFrequencies(cfg.head_dim, cfg.max_seq_len, cfg.theta, cfg.device)\n",
    "    freqs = precompute_freqs()\n",
    "else:\n",
    "    freqs = None\n",
    "mask = torch.ones(cfg.max_seq_len, cfg.max_seq_len, dtype=torch.bool, device=cfg.device).triu(diagonal=1)\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "output = module(x, freqs, mask, training=True)\n",
    "module.disable_logging()\n",
    "del module,freqs, mask, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f534ab4-a150-42db-8c89-5e8d3d0c06a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.73K\n",
      "Layer(\n",
      "  (pre_attn_norm): Norm()\n",
      "  (attn): SelfAttention(\n",
      "    (Wq): Linear(in_features=8, out_features=8, bias=False)\n",
      "    (Wk): Linear(in_features=8, out_features=4, bias=False)\n",
      "    (Wv): Linear(in_features=8, out_features=4, bias=False)\n",
      "    (Wo): Linear(in_features=8, out_features=8, bias=False)\n",
      "  )\n",
      "  (pre_mlp_norm): Norm()\n",
      "  (mlp): MLP(\n",
      "    (Wup): Linear(in_features=8, out_features=21, bias=False)\n",
      "    (Wgate): Linear(in_features=8, out_features=21, bias=False)\n",
      "    (Wdown): Linear(in_features=21, out_features=8, bias=False)\n",
      "    (nonlinearity): Mish()\n",
      "  )\n",
      ")\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 2, 8]), dtype: torch.float32, device: mps:0, min/max: -3.180/2.101\n",
      "Other-type 'freqs': Type=NoneType, Value=None\n",
      "Tensor 'mask' shape: torch.Size([2, 5]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Integer 'cache_len': Value=3\n",
      "Dict 'kv_cache':\n",
      "    Tensor 'kv_cache[k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Tensor 'kv_cache[v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "Bool 'training': Value=False\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 2, 8]), dtype: torch.float32, device: mps:0, min/max: -3.180/2.101\n",
      "Other-type 'freqs': Type=NoneType, Value=None\n",
      "Tensor 'mask' shape: torch.Size([2, 5]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Integer 'cache_len': Value=3\n",
      "Dict 'kv_cache':\n",
      "    Tensor 'kv_cache[k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Tensor 'kv_cache[v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "Bool 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 2, 8]), dtype: torch.float32, device: mps:0, min/max: -0.156/0.141\n",
      "Dict 'output[1]':\n",
      "    Tensor 'output[1][k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.061/1.118\n",
      "    Tensor 'output[1][v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.218/0.997\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 2, 8]), dtype: torch.float32, device: mps:0, min/max: -3.220/1.970\n",
      "Bool 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 2, 8]), dtype: torch.float32, device: mps:0, min/max: -0.370/0.376\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 2, 8]), dtype: torch.float32, device: mps:0, min/max: -3.210/1.902\n",
      "Dict 'output[1]':\n",
      "    Tensor 'output[1][k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.061/1.118\n",
      "    Tensor 'output[1][v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.218/0.997\n",
      "====================Exiting Layer.forward====================\n",
      "CPU times: user 59.2 ms, sys: 8.12 ms, total: 67.3 ms\n",
      "Wall time: 74.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# INFERENCE\n",
    "module = Layer(cfg).to(cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "#module.disable_function_logging('attn_connect')\n",
    "#module.disable_function_logging('mlp_connect')\n",
    "#module.pre_attn_norm.enable_logging()\n",
    "#module.attn.enable_logging()\n",
    "#module.post_attn_norm.enable_logging()\n",
    "#module.pre_mlp_norm.enable_logging()\n",
    "#module.mlp.enable_logging()\n",
    "#module.post_mlp_norm.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "if cfg.pos_enc_type == 'RoPE':\n",
    "    precompute_freqs = PrecomputeRotaryFrequencies(cfg.head_dim, cfg.max_seq_len, cfg.theta, cfg.device)\n",
    "    freqs = precompute_freqs()\n",
    "else:\n",
    "    freqs = None\n",
    "mask = torch.ones(cfg.max_seq_len, cfg.max_seq_len, dtype=torch.bool, device=cfg.device).triu(diagonal=1)\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "# setting up for kv caching\n",
    "cache_len = cfg.max_seq_len // 3\n",
    "context_chunk_len = cfg.max_seq_len // 4\n",
    "seq_len = cache_len + context_chunk_len\n",
    "kv_cache = {\n",
    "    'k': torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device),\n",
    "    'v': torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device)\n",
    "}\n",
    "# need to extend the mask with zeros for the cached values\n",
    "mask = torch.nn.functional.pad(mask[:context_chunk_len, :context_chunk_len], (cache_len, 0, 0, 0), value=False).bool()\n",
    "# these don't use seq_len because those entries should already be in the kv cache\n",
    "#freqs_cis = freqs_cis[:context_chunk_len]\n",
    "x = torch.randn(tcfg.micro_batch_size,context_chunk_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "output = module(x, freqs, mask, cache_len, kv_cache)\n",
    "module.disable_logging()\n",
    "del module, freqs, mask, cache_len, context_chunk_len, seq_len, kv_cache, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c677ac2b-06d0-4895-b718-2bc664613c98",
   "metadata": {},
   "source": [
    "# Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "655a1fec-4e32-4c7a-86e0-1390a03e88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ccc88d4-c650-43d3-85ee-e68152cb2e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.01M\n",
      "Model(\n",
      "  (token_embedder): Embedding(512, 8)\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): SelfAttention(\n",
      "        (Wq): Linear(in_features=8, out_features=8, bias=False)\n",
      "        (Wk): Linear(in_features=8, out_features=4, bias=False)\n",
      "        (Wv): Linear(in_features=8, out_features=4, bias=False)\n",
      "        (Wo): Linear(in_features=8, out_features=8, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wup): Linear(in_features=8, out_features=21, bias=False)\n",
      "        (Wgate): Linear(in_features=8, out_features=21, bias=False)\n",
      "        (Wdown): Linear(in_features=21, out_features=8, bias=False)\n",
      "        (nonlinearity): Mish()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (output): Linear(in_features=8, out_features=512, bias=False)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "\n",
      "====================Entering Model.forward====================\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([4, 10]), dtype: torch.int64, device: mps:0, min/max: 16.000/511.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Tensor 'target_token_ids' shape: torch.Size([4, 10]), dtype: torch.int64, device: mps:0, min/max: 1.000/505.000\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -3.158/3.272\n",
      "Other-type 'freqs': Type=NoneType, Value=None\n",
      "Tensor 'mask' shape: torch.Size([10, 10]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -3.158/3.272\n",
      "Other-type 'freqs': Type=NoneType, Value=None\n",
      "Tensor 'mask' shape: torch.Size([10, 10]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.004/0.002\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -3.158/3.272\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.000/0.000\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -3.158/3.272\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.forward====================\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -3.158/3.272\n",
      "Other-type 'freqs': Type=NoneType, Value=None\n",
      "Tensor 'mask' shape: torch.Size([10, 10]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -3.158/3.272\n",
      "Other-type 'freqs': Type=NoneType, Value=None\n",
      "Tensor 'mask' shape: torch.Size([10, 10]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.002/0.001\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -3.158/3.271\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.000/0.000\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -3.158/3.271\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.forward====================\n",
      "\n",
      "====================Entering Norm.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -3.158/3.271\n",
      "\n",
      "====================Entering Norm.RMSNorm====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -3.158/3.271\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.206/1.969\n",
      "====================Exiting Norm.RMSNorm====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.206/1.969\n",
      "====================Exiting Norm.forward====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 10, 512]), dtype: torch.float32, device: mps:0, min/max: -0.189/0.202\n",
      "Tensor 'output[1]' shape: torch.Size([]), dtype: torch.float32, device: mps:0, min/max: 6.251/6.251\n",
      "====================Exiting Model.forward====================\n",
      "tensor(6.2508, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "CPU times: user 121 ms, sys: 18.6 ms, total: 139 ms\n",
      "Wall time: 366 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TRAINING\n",
    "module = Model(cfg).to(cfg.device)\n",
    "print(\"number of parameters: %.2fM\" % (module.get_num_params()/1e6,))\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "### enabling printing for sub-modules\n",
    "#module.precompute_freqs.enable_logging() # only un-comment this line if using RoPE\n",
    "#module.layers[0].enable_logging()\n",
    "for i in range(cfg.num_layers):\n",
    "    module.layers[i].enable_logging()\n",
    "module.final_norm.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "input_token_ids = torch.randint(tokenizer.vocab_len, (tcfg.micro_batch_size, cfg.max_seq_len)).to(cfg.device)\n",
    "target_token_ids = torch.randint(tokenizer.vocab_len, (tcfg.micro_batch_size, cfg.max_seq_len)).to(cfg.device)\n",
    "\n",
    "output, loss = module(input_token_ids, target_token_ids=target_token_ids)\n",
    "print(loss)\n",
    "del module, input_token_ids, target_token_ids, output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "316598a6-1ba9-4a26-962f-16c0802a698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.01M\n",
      "Model(\n",
      "  (token_embedder): Embedding(512, 8)\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): SelfAttention(\n",
      "        (Wq): Linear(in_features=8, out_features=8, bias=False)\n",
      "        (Wk): Linear(in_features=8, out_features=4, bias=False)\n",
      "        (Wv): Linear(in_features=8, out_features=4, bias=False)\n",
      "        (Wo): Linear(in_features=8, out_features=8, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wup): Linear(in_features=8, out_features=21, bias=False)\n",
      "        (Wgate): Linear(in_features=8, out_features=21, bias=False)\n",
      "        (Wdown): Linear(in_features=21, out_features=8, bias=False)\n",
      "        (nonlinearity): Mish()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (output): Linear(in_features=8, out_features=512, bias=False)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "\n",
      "====================Entering Model.forward====================\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([4, 2]), dtype: torch.int64, device: mps:0, min/max: 56.000/467.000\n",
      "Integer 'cache_len': Value=3\n",
      "List 'kv_cache':\n",
      "    Dict 'kv_cache[0]':\n",
      "        Tensor 'kv_cache[0][k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "        Tensor 'kv_cache[0][v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Dict 'kv_cache[1]':\n",
      "        Tensor 'kv_cache[1][k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "        Tensor 'kv_cache[1][v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "Other-type 'target_token_ids': Type=NoneType, Value=None\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 2, 512]), dtype: torch.float32, device: mps:0, min/max: -0.159/0.176\n",
      "List 'output[1]':\n",
      "    Dict 'output[1][0]':\n",
      "        Tensor 'output[1][0][k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.044/0.032\n",
      "        Tensor 'output[1][0][v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.078/0.095\n",
      "    Dict 'output[1][1]':\n",
      "        Tensor 'output[1][1][k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.092/0.102\n",
      "        Tensor 'output[1][1][v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.072/0.009\n",
      "====================Exiting Model.forward====================\n",
      "CPU times: user 27.9 ms, sys: 4.09 ms, total: 32 ms\n",
      "Wall time: 36 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Inference\n",
    "module = Model(cfg).to(cfg.device)\n",
    "print(\"number of parameters: %.2fM\" % (module.get_num_params()/1e6,))\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "### enabling printing for sub-modules\n",
    "#for i in range(cfg.num_layers):\n",
    "    #module.layers[i].enable_logging()\n",
    "#module.final_norm.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "input_token_ids = torch.randint(tokenizer.vocab_len, (tcfg.micro_batch_size, cfg.max_seq_len // 4)).to(cfg.device)\n",
    "kv_cache = [{ # Initialize kv caches for each layer\n",
    "                \"k\": torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device),\n",
    "                \"v\": torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device),\n",
    "            } for _ in range(cfg.num_layers)]\n",
    "\n",
    "output, kv_cache = module(input_token_ids, cache_len = cfg.max_seq_len // 3, kv_cache = kv_cache)\n",
    "\n",
    "del module, input_token_ids, kv_cache, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d82f9c-e531-47b5-8ffb-c8374ffb0e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
