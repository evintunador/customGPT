{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea126987-59aa-4f76-b926-6d632887c30b",
   "metadata": {},
   "source": [
    "# This notebook is designed for teaching/testing purposes to help you visualize the tensor shapes that go through each module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f833b1f8-ea91-4ae5-b3a3-73e08e4c8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that. \n",
    "# you prolly won't need this cell but running it won't hurt anything either\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c2c04f-2dbd-4020-8d91-cc0e4e8511b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelConfig(dim=8, device='mps', out_weight_share=True, linear_bias=False, tokenizer='bpe_tinyStories', vocab_len=512, num_layers=2, second_resid_norm=False, mlp_hidden_mult=4, mlp_nonlinearity='SiLU', mlp_gated=True, num_q_heads=2, num_kv_heads=1, head_dim=4, theta=10000, max_seq_len=10, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06)\n",
      "TrainConfig(model_name='2024-06-30|21-25-01', dataset_name='noanabeshima/TinyStoriesV2', data_subset=None, streaming=False, micro_batch_size=4, grad_accum_steps=2, max_iters=4, eval_interval=2, eval_samples=1, checkpoint_interval=None, beta1=0.9, beta2=0.95, epsilon=1e-08, weight_decay=0.05, grad_clip=1.0, lr_init=1e-06, lr_max=0.01, lr_min=0.0001, warmup_iters=0, final_flat_iters=0, anneal_type='cos', num_restarts=0, T_mult=2)\n"
     ]
    }
   ],
   "source": [
    "# config file\n",
    "from config import ModelConfig, TrainConfig\n",
    "cfg = ModelConfig()\n",
    "tcfg = TrainConfig()\n",
    "print(cfg)\n",
    "print(tcfg)\n",
    "\n",
    "# import the tokenizer specified by cfg\n",
    "from tools import import_from_nested_path\n",
    "imported_objects = import_from_nested_path(['custom_tokenizers', cfg.tokenizer], 'tokenizer', ['get_tokenizer'])\n",
    "get_tokenizer = imported_objects.get('get_tokenizer')\n",
    "tokenizer = get_tokenizer(size = 512) # assuming 'bpe', size options are 512, 1024 and 2048\n",
    "\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c12e3b-dc63-4479-ad55-b05d96364d1f",
   "metadata": {},
   "source": [
    "# Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3debc7b1-a7ec-4fb3-98cb-d16edf7c71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.norm import Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627969e9-9017-43f3-90ec-9a485abef26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.02K\n",
      "Norm()\n",
      "\n",
      "====================Entering Norm.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.465/2.547\n",
      "\n",
      "====================Entering Norm.RMSNorm====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.465/2.547\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.294/2.624\n",
      "====================Exiting Norm.RMSNorm====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.294/2.624\n",
      "====================Exiting Norm.forward====================\n",
      "CPU times: user 62.6 ms, sys: 40 ms, total: 103 ms\n",
      "Wall time: 110 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### RMSNorm\n",
    "\n",
    "# Create an instance of RMSNorm\n",
    "module = Norm(cfg.dim, 'RMSNorm').to(cfg.device)\n",
    "\n",
    "# let's take a look\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "\n",
    "# Initially, logging is disabled by default\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('CosineNorm')\n",
    "#module.disable_function_logging('LayerNorm')\n",
    "#module.disable_function_logging('RMSNorm')\n",
    "\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "output = module(x)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64020a96-205a-45b7-ae56-a3555d2b3719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================Entering Norm.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.912/2.715\n",
      "Full tensor content:\n",
      "tensor([[[ 1.2539e+00,  1.1740e+00, -3.1399e-01, -4.2712e-02, -1.2338e+00,\n",
      "          -5.3306e-01, -8.4329e-02, -3.9619e-01],\n",
      "         [-4.9208e-02, -7.3625e-01, -1.8756e+00,  3.2498e-01, -1.1523e+00,\n",
      "          -1.0863e+00, -9.6733e-01,  6.0719e-01],\n",
      "         [-5.8460e-01,  5.9410e-01, -9.7479e-01, -4.2559e-01,  1.8048e+00,\n",
      "          -3.7047e-01,  8.9476e-02,  1.7904e+00],\n",
      "         [ 1.8794e-01, -8.1233e-01, -9.5944e-01, -9.0259e-01, -1.4441e+00,\n",
      "          -1.9472e+00, -3.1793e-01,  1.0997e+00],\n",
      "         [ 6.5820e-01,  2.2273e+00, -1.4156e+00,  1.5682e-02, -1.3306e-01,\n",
      "           6.6103e-01, -2.6299e-01, -2.3854e-01],\n",
      "         [ 5.4644e-02,  3.9237e-01, -6.1842e-01, -7.6074e-01, -3.7094e-01,\n",
      "          -6.9196e-01,  1.7675e+00,  3.7976e-01],\n",
      "         [ 9.1040e-03,  5.2133e-01,  3.6259e-01, -2.5275e-01,  6.6164e-01,\n",
      "           8.2407e-01, -5.6123e-01, -1.1834e+00],\n",
      "         [-4.6213e-02, -1.3745e+00, -1.4806e+00,  3.4358e-01,  3.5904e-01,\n",
      "          -5.4951e-01,  2.2126e-02,  7.2588e-01],\n",
      "         [ 1.0279e+00,  1.3197e+00, -2.7582e-01,  3.6966e-02, -2.8416e-01,\n",
      "           1.5792e+00,  1.0671e+00,  2.6019e+00],\n",
      "         [-8.4078e-01, -1.5097e-01,  1.5901e-01,  4.7197e-01,  4.5772e-01,\n",
      "           1.0006e+00,  1.1570e+00, -5.5262e-01]],\n",
      "\n",
      "        [[-2.1619e-01, -1.0961e-01, -1.0803e+00,  1.9492e-01,  8.4174e-01,\n",
      "          -9.1881e-02, -1.5738e-01, -9.8494e-01],\n",
      "         [ 7.6206e-01,  8.6491e-01, -1.2071e+00, -1.1989e+00,  7.0462e-01,\n",
      "          -2.0171e-01,  4.6657e-01,  1.2163e-01],\n",
      "         [-9.4645e-01, -1.7218e+00,  1.6129e+00,  1.3036e-02, -1.9897e+00,\n",
      "           1.2640e+00,  2.7150e+00, -1.2312e+00],\n",
      "         [-2.6691e-02,  1.4160e-01, -6.3416e-02,  2.4658e+00, -6.0744e-02,\n",
      "          -4.6279e-01,  4.2127e-01,  9.6265e-01],\n",
      "         [-6.5434e-01,  1.8508e+00,  7.1183e-01, -6.9078e-01, -5.4932e-01,\n",
      "           3.5080e-01, -1.2157e+00, -1.7757e+00],\n",
      "         [ 1.3650e+00, -6.3488e-01, -1.5334e+00,  2.7707e-01,  8.1093e-01,\n",
      "           1.3768e+00,  1.7557e+00, -1.0239e+00],\n",
      "         [-6.4215e-01,  3.5488e-01, -7.1369e-01,  1.1501e+00, -1.8599e-01,\n",
      "          -7.0545e-01, -6.2320e-01,  6.6215e-01],\n",
      "         [-1.8040e-02, -1.0017e-01,  1.6530e-01,  1.4471e-01, -3.5491e-01,\n",
      "          -7.8699e-01, -7.1911e-02,  4.1613e-01],\n",
      "         [ 1.2060e+00,  1.8918e+00,  1.0394e+00, -8.4831e-01,  1.0566e+00,\n",
      "          -1.2592e+00,  6.7812e-01,  9.8670e-01],\n",
      "         [ 8.2923e-01,  1.1987e-01,  5.4352e-01,  1.2938e-01, -8.3195e-01,\n",
      "           2.2857e+00,  1.9933e-01,  1.4788e+00]],\n",
      "\n",
      "        [[-4.0196e-01,  1.4018e+00, -1.2214e+00,  6.8511e-01,  9.9210e-01,\n",
      "           1.8613e-01,  1.4124e+00,  1.1132e-03],\n",
      "         [ 5.6327e-01,  1.2495e+00,  1.7993e-01, -7.7559e-01,  2.6728e-01,\n",
      "           1.5025e-02, -9.5637e-01,  1.2117e+00],\n",
      "         [ 8.0547e-01, -2.0074e-01,  1.9891e+00, -6.2149e-01,  2.5594e-01,\n",
      "           9.8821e-01, -1.0274e+00,  2.1718e+00],\n",
      "         [ 6.1548e-01,  6.7938e-01, -1.4289e-02,  7.2741e-01, -3.4721e-01,\n",
      "           7.9543e-01,  3.6305e-01, -9.0062e-01],\n",
      "         [-1.2989e-01,  6.6693e-01, -5.2662e-01, -3.5840e-01, -3.1570e-01,\n",
      "          -2.2784e-01,  1.1804e+00, -7.9302e-01],\n",
      "         [-2.4701e+00,  3.6529e-01,  3.9226e-01, -2.0117e+00, -6.7146e-01,\n",
      "          -1.4421e+00, -1.2780e+00,  1.5897e+00],\n",
      "         [-1.0904e+00, -6.2342e-01,  2.0262e-01, -4.5975e-01, -6.3706e-01,\n",
      "          -1.3688e+00,  1.4720e-01, -1.0041e+00],\n",
      "         [ 1.7752e+00,  1.6961e+00,  3.6764e-01, -1.3099e+00,  1.1685e+00,\n",
      "          -5.9923e-01,  7.6328e-01, -1.7661e+00],\n",
      "         [ 4.8759e-01,  5.0499e-01,  8.4199e-01, -8.5640e-01, -7.6422e-01,\n",
      "           2.6785e-01,  1.2884e+00, -5.7037e-01],\n",
      "         [ 2.0228e-01,  1.4139e+00, -2.5258e-01,  7.2949e-01,  4.3023e-01,\n",
      "           9.1155e-01,  5.6768e-01,  1.4107e+00]],\n",
      "\n",
      "        [[ 4.8786e-01,  2.1926e+00, -7.4882e-01,  1.1497e+00,  8.7120e-02,\n",
      "          -4.1618e-01, -1.2741e+00,  2.0626e+00],\n",
      "         [-1.1824e+00, -5.1753e-01, -1.8548e+00,  7.6860e-01,  1.7203e+00,\n",
      "          -1.7607e-01,  1.2898e+00, -1.7418e-01],\n",
      "         [-4.1850e-01,  4.3095e-01,  6.6258e-02, -1.1161e+00,  1.3554e+00,\n",
      "           8.2423e-01,  6.1935e-01,  2.6459e-01],\n",
      "         [-2.0388e-01,  4.7982e-01,  1.4370e+00, -5.1386e-01,  3.4779e-01,\n",
      "           7.2283e-01,  7.6451e-01, -6.2807e-02],\n",
      "         [ 1.8420e+00, -4.7747e-01,  2.2311e+00, -6.7254e-01,  2.1664e+00,\n",
      "           4.3439e-01,  6.4667e-01,  1.2713e-01],\n",
      "         [-4.3787e-02,  2.1627e-01,  1.2830e+00,  4.3170e-02, -1.0580e+00,\n",
      "          -2.3925e-01,  6.0586e-01,  1.2971e+00],\n",
      "         [-5.6255e-01, -8.8890e-02, -4.1844e-01, -9.3539e-01, -1.4716e-02,\n",
      "          -3.4423e-02, -1.4583e+00, -1.3102e-01],\n",
      "         [-1.7388e+00, -3.6997e-01, -3.2814e-01, -1.5046e-01, -8.9166e-01,\n",
      "          -2.9120e+00, -3.4263e-02, -1.8481e-01],\n",
      "         [ 7.8260e-01, -8.9459e-01,  9.0216e-01,  9.8000e-02,  1.9463e-01,\n",
      "           3.9380e-01,  2.0306e-01,  1.9250e-03],\n",
      "         [-3.4106e-01, -7.2638e-01,  5.4761e-01, -3.5825e-01, -3.0559e-02,\n",
      "          -4.6995e-01, -3.8622e-01,  1.2643e-02]]], device='mps:0')\n",
      "\n",
      "====================Entering Norm.LayerNorm====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.912/2.715\n",
      "Full tensor content:\n",
      "tensor([[[ 1.2539e+00,  1.1740e+00, -3.1399e-01, -4.2712e-02, -1.2338e+00,\n",
      "          -5.3306e-01, -8.4329e-02, -3.9619e-01],\n",
      "         [-4.9208e-02, -7.3625e-01, -1.8756e+00,  3.2498e-01, -1.1523e+00,\n",
      "          -1.0863e+00, -9.6733e-01,  6.0719e-01],\n",
      "         [-5.8460e-01,  5.9410e-01, -9.7479e-01, -4.2559e-01,  1.8048e+00,\n",
      "          -3.7047e-01,  8.9476e-02,  1.7904e+00],\n",
      "         [ 1.8794e-01, -8.1233e-01, -9.5944e-01, -9.0259e-01, -1.4441e+00,\n",
      "          -1.9472e+00, -3.1793e-01,  1.0997e+00],\n",
      "         [ 6.5820e-01,  2.2273e+00, -1.4156e+00,  1.5682e-02, -1.3306e-01,\n",
      "           6.6103e-01, -2.6299e-01, -2.3854e-01],\n",
      "         [ 5.4644e-02,  3.9237e-01, -6.1842e-01, -7.6074e-01, -3.7094e-01,\n",
      "          -6.9196e-01,  1.7675e+00,  3.7976e-01],\n",
      "         [ 9.1040e-03,  5.2133e-01,  3.6259e-01, -2.5275e-01,  6.6164e-01,\n",
      "           8.2407e-01, -5.6123e-01, -1.1834e+00],\n",
      "         [-4.6213e-02, -1.3745e+00, -1.4806e+00,  3.4358e-01,  3.5904e-01,\n",
      "          -5.4951e-01,  2.2126e-02,  7.2588e-01],\n",
      "         [ 1.0279e+00,  1.3197e+00, -2.7582e-01,  3.6966e-02, -2.8416e-01,\n",
      "           1.5792e+00,  1.0671e+00,  2.6019e+00],\n",
      "         [-8.4078e-01, -1.5097e-01,  1.5901e-01,  4.7197e-01,  4.5772e-01,\n",
      "           1.0006e+00,  1.1570e+00, -5.5262e-01]],\n",
      "\n",
      "        [[-2.1619e-01, -1.0961e-01, -1.0803e+00,  1.9492e-01,  8.4174e-01,\n",
      "          -9.1881e-02, -1.5738e-01, -9.8494e-01],\n",
      "         [ 7.6206e-01,  8.6491e-01, -1.2071e+00, -1.1989e+00,  7.0462e-01,\n",
      "          -2.0171e-01,  4.6657e-01,  1.2163e-01],\n",
      "         [-9.4645e-01, -1.7218e+00,  1.6129e+00,  1.3036e-02, -1.9897e+00,\n",
      "           1.2640e+00,  2.7150e+00, -1.2312e+00],\n",
      "         [-2.6691e-02,  1.4160e-01, -6.3416e-02,  2.4658e+00, -6.0744e-02,\n",
      "          -4.6279e-01,  4.2127e-01,  9.6265e-01],\n",
      "         [-6.5434e-01,  1.8508e+00,  7.1183e-01, -6.9078e-01, -5.4932e-01,\n",
      "           3.5080e-01, -1.2157e+00, -1.7757e+00],\n",
      "         [ 1.3650e+00, -6.3488e-01, -1.5334e+00,  2.7707e-01,  8.1093e-01,\n",
      "           1.3768e+00,  1.7557e+00, -1.0239e+00],\n",
      "         [-6.4215e-01,  3.5488e-01, -7.1369e-01,  1.1501e+00, -1.8599e-01,\n",
      "          -7.0545e-01, -6.2320e-01,  6.6215e-01],\n",
      "         [-1.8040e-02, -1.0017e-01,  1.6530e-01,  1.4471e-01, -3.5491e-01,\n",
      "          -7.8699e-01, -7.1911e-02,  4.1613e-01],\n",
      "         [ 1.2060e+00,  1.8918e+00,  1.0394e+00, -8.4831e-01,  1.0566e+00,\n",
      "          -1.2592e+00,  6.7812e-01,  9.8670e-01],\n",
      "         [ 8.2923e-01,  1.1987e-01,  5.4352e-01,  1.2938e-01, -8.3195e-01,\n",
      "           2.2857e+00,  1.9933e-01,  1.4788e+00]],\n",
      "\n",
      "        [[-4.0196e-01,  1.4018e+00, -1.2214e+00,  6.8511e-01,  9.9210e-01,\n",
      "           1.8613e-01,  1.4124e+00,  1.1132e-03],\n",
      "         [ 5.6327e-01,  1.2495e+00,  1.7993e-01, -7.7559e-01,  2.6728e-01,\n",
      "           1.5025e-02, -9.5637e-01,  1.2117e+00],\n",
      "         [ 8.0547e-01, -2.0074e-01,  1.9891e+00, -6.2149e-01,  2.5594e-01,\n",
      "           9.8821e-01, -1.0274e+00,  2.1718e+00],\n",
      "         [ 6.1548e-01,  6.7938e-01, -1.4289e-02,  7.2741e-01, -3.4721e-01,\n",
      "           7.9543e-01,  3.6305e-01, -9.0062e-01],\n",
      "         [-1.2989e-01,  6.6693e-01, -5.2662e-01, -3.5840e-01, -3.1570e-01,\n",
      "          -2.2784e-01,  1.1804e+00, -7.9302e-01],\n",
      "         [-2.4701e+00,  3.6529e-01,  3.9226e-01, -2.0117e+00, -6.7146e-01,\n",
      "          -1.4421e+00, -1.2780e+00,  1.5897e+00],\n",
      "         [-1.0904e+00, -6.2342e-01,  2.0262e-01, -4.5975e-01, -6.3706e-01,\n",
      "          -1.3688e+00,  1.4720e-01, -1.0041e+00],\n",
      "         [ 1.7752e+00,  1.6961e+00,  3.6764e-01, -1.3099e+00,  1.1685e+00,\n",
      "          -5.9923e-01,  7.6328e-01, -1.7661e+00],\n",
      "         [ 4.8759e-01,  5.0499e-01,  8.4199e-01, -8.5640e-01, -7.6422e-01,\n",
      "           2.6785e-01,  1.2884e+00, -5.7037e-01],\n",
      "         [ 2.0228e-01,  1.4139e+00, -2.5258e-01,  7.2949e-01,  4.3023e-01,\n",
      "           9.1155e-01,  5.6768e-01,  1.4107e+00]],\n",
      "\n",
      "        [[ 4.8786e-01,  2.1926e+00, -7.4882e-01,  1.1497e+00,  8.7120e-02,\n",
      "          -4.1618e-01, -1.2741e+00,  2.0626e+00],\n",
      "         [-1.1824e+00, -5.1753e-01, -1.8548e+00,  7.6860e-01,  1.7203e+00,\n",
      "          -1.7607e-01,  1.2898e+00, -1.7418e-01],\n",
      "         [-4.1850e-01,  4.3095e-01,  6.6258e-02, -1.1161e+00,  1.3554e+00,\n",
      "           8.2423e-01,  6.1935e-01,  2.6459e-01],\n",
      "         [-2.0388e-01,  4.7982e-01,  1.4370e+00, -5.1386e-01,  3.4779e-01,\n",
      "           7.2283e-01,  7.6451e-01, -6.2807e-02],\n",
      "         [ 1.8420e+00, -4.7747e-01,  2.2311e+00, -6.7254e-01,  2.1664e+00,\n",
      "           4.3439e-01,  6.4667e-01,  1.2713e-01],\n",
      "         [-4.3787e-02,  2.1627e-01,  1.2830e+00,  4.3170e-02, -1.0580e+00,\n",
      "          -2.3925e-01,  6.0586e-01,  1.2971e+00],\n",
      "         [-5.6255e-01, -8.8890e-02, -4.1844e-01, -9.3539e-01, -1.4716e-02,\n",
      "          -3.4423e-02, -1.4583e+00, -1.3102e-01],\n",
      "         [-1.7388e+00, -3.6997e-01, -3.2814e-01, -1.5046e-01, -8.9166e-01,\n",
      "          -2.9120e+00, -3.4263e-02, -1.8481e-01],\n",
      "         [ 7.8260e-01, -8.9459e-01,  9.0216e-01,  9.8000e-02,  1.9463e-01,\n",
      "           3.9380e-01,  2.0306e-01,  1.9250e-03],\n",
      "         [-3.4106e-01, -7.2638e-01,  5.4761e-01, -3.5825e-01, -3.0559e-02,\n",
      "          -4.6995e-01, -3.8622e-01,  1.2643e-02]]], device='mps:0')\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.208/2.360\n",
      "Full tensor content:\n",
      "tensor([[[ 1.6111e+00,  1.5102e+00, -3.6867e-01, -2.6119e-02, -1.5301e+00,\n",
      "          -6.4529e-01, -7.8669e-02, -4.7247e-01],\n",
      "         [ 7.2265e-01, -1.5199e-01, -1.6025e+00,  1.1990e+00, -6.8164e-01,\n",
      "          -5.9765e-01, -4.4616e-01,  1.5583e+00],\n",
      "         [-8.2648e-01,  3.5433e-01, -1.2174e+00, -6.6718e-01,  1.5671e+00,\n",
      "          -6.1196e-01, -1.5120e-01,  1.5527e+00],\n",
      "         [ 9.2433e-01, -1.9646e-01, -3.6129e-01, -2.9759e-01, -9.0440e-01,\n",
      "          -1.4681e+00,  3.5750e-01,  1.9460e+00],\n",
      "         [ 4.7846e-01,  2.0785e+00, -1.6363e+00, -1.7674e-01, -3.2842e-01,\n",
      "           4.8134e-01, -4.6091e-01, -4.3598e-01],\n",
      "         [ 4.5061e-02,  4.7226e-01, -8.0632e-01, -9.8635e-01, -4.9328e-01,\n",
      "          -8.9935e-01,  2.2117e+00,  4.5631e-01],\n",
      "         [-6.0214e-02,  7.3958e-01,  4.9173e-01, -4.6907e-01,  9.5866e-01,\n",
      "           1.2123e+00, -9.5075e-01, -1.9222e+00],\n",
      "         [ 2.6705e-01, -1.4734e+00, -1.6124e+00,  7.7779e-01,  7.9804e-01,\n",
      "          -3.9241e-01,  3.5659e-01,  1.2787e+00],\n",
      "         [ 1.5256e-01,  4.6222e-01, -1.2307e+00, -8.9884e-01, -1.2396e+00,\n",
      "           7.3754e-01,  1.9413e-01,  1.8227e+00],\n",
      "         [-1.5996e+00, -5.5221e-01, -8.1570e-02,  3.9360e-01,  3.7196e-01,\n",
      "           1.1962e+00,  1.4336e+00, -1.1620e+00]],\n",
      "\n",
      "        [[-2.7327e-02,  1.5771e-01, -1.5275e+00,  6.8642e-01,  1.8094e+00,\n",
      "           1.8849e-01,  7.4785e-02, -1.3620e+00],\n",
      "         [ 9.1636e-01,  1.0467e+00, -1.5793e+00, -1.5689e+00,  8.4356e-01,\n",
      "          -3.0507e-01,  5.4186e-01,  1.0471e-01],\n",
      "         [-5.6364e-01, -1.0434e+00,  1.0200e+00,  3.0044e-02, -1.2091e+00,\n",
      "           8.0410e-01,  1.7019e+00, -7.3985e-01],\n",
      "         [-5.1831e-01, -3.2400e-01, -5.6071e-01,  2.3596e+00, -5.5762e-01,\n",
      "          -1.0218e+00, -1.0813e-03,  6.2399e-01],\n",
      "         [-3.7606e-01,  1.9341e+00,  8.8381e-01, -4.0966e-01, -2.7921e-01,\n",
      "           5.5087e-01, -8.9373e-01, -1.4102e+00],\n",
      "         [ 9.2323e-01, -8.0907e-01, -1.5873e+00, -1.9133e-02,  4.4330e-01,\n",
      "           9.3343e-01,  1.2616e+00, -1.1461e+00],\n",
      "         [-8.1855e-01,  6.5396e-01, -9.2419e-01,  1.8285e+00, -1.4485e-01,\n",
      "          -9.1203e-01, -7.9056e-01,  1.1078e+00],\n",
      "         [ 1.6864e-01, -7.1432e-02,  7.0453e-01,  6.4434e-01, -8.1599e-01,\n",
      "          -2.0789e+00,  1.1180e-02,  1.4377e+00],\n",
      "         [ 6.0656e-01,  1.2862e+00,  4.4143e-01, -1.4291e+00,  4.5853e-01,\n",
      "          -1.8363e+00,  8.3470e-02,  3.8925e-01],\n",
      "         [ 2.6412e-01, -5.3318e-01, -5.7006e-02, -5.2249e-01, -1.6030e+00,\n",
      "           1.9012e+00, -4.4387e-01,  9.9422e-01]],\n",
      "\n",
      "        [[-9.1009e-01,  1.1841e+00, -1.8615e+00,  3.5202e-01,  7.0845e-01,\n",
      "          -2.2730e-01,  1.1964e+00, -4.4211e-01],\n",
      "         [ 4.5444e-01,  1.3611e+00, -5.2070e-02, -1.3146e+00,  6.3341e-02,\n",
      "          -2.6997e-01, -1.5535e+00,  1.3113e+00],\n",
      "         [ 2.3931e-01, -6.8556e-01,  1.3273e+00, -1.0723e+00, -2.6580e-01,\n",
      "           4.0728e-01, -1.4454e+00,  1.4952e+00],\n",
      "         [ 6.5820e-01,  7.7018e-01, -4.4526e-01,  8.5433e-01, -1.0286e+00,\n",
      "           9.7351e-01,  2.1590e-01, -1.9983e+00],\n",
      "         [-1.0903e-01,  1.1901e+00, -7.5586e-01, -4.8160e-01, -4.1198e-01,\n",
      "          -2.6873e-01,  2.0273e+00, -1.1902e+00],\n",
      "         [-1.3795e+00,  8.1876e-01,  8.3967e-01, -1.0241e+00,  1.4969e-02,\n",
      "          -5.8254e-01, -4.5526e-01,  1.7680e+00],\n",
      "         [-9.2314e-01, -3.6483e-02,  1.5321e+00,  2.7430e-01, -6.2384e-02,\n",
      "          -1.4518e+00,  1.4268e+00, -7.5937e-01],\n",
      "         [ 1.1984e+00,  1.1358e+00,  8.3714e-02, -1.2448e+00,  7.1792e-01,\n",
      "          -6.8196e-01,  3.9703e-01, -1.6060e+00],\n",
      "         [ 4.5543e-01,  4.7890e-01,  9.3351e-01, -1.3576e+00, -1.2332e+00,\n",
      "           1.5901e-01,  1.5357e+00, -9.7174e-01],\n",
      "         [-8.8444e-01,  1.3745e+00, -1.7325e+00,  9.8504e-02, -4.5944e-01,\n",
      "           4.3795e-01, -2.0317e-01,  1.3686e+00]],\n",
      "\n",
      "        [[ 3.7880e-02,  1.4647e+00, -9.9721e-01,  5.9185e-01, -2.9754e-01,\n",
      "          -7.1879e-01, -1.4369e+00,  1.3559e+00],\n",
      "         [-1.0242e+00, -4.4049e-01, -1.6146e+00,  6.8863e-01,  1.5241e+00,\n",
      "          -1.4072e-01,  1.1462e+00, -1.3906e-01],\n",
      "         [-9.4205e-01,  2.4916e-01, -2.6226e-01, -1.9204e+00,  1.5456e+00,\n",
      "           8.0067e-01,  5.1336e-01,  1.5872e-02],\n",
      "         [-9.8502e-01,  1.8559e-01,  1.8244e+00, -1.5157e+00, -4.0461e-02,\n",
      "           6.0168e-01,  6.7304e-01, -7.4347e-01],\n",
      "         [ 9.7255e-01, -1.1661e+00,  1.3314e+00, -1.3460e+00,  1.2717e+00,\n",
      "          -3.2532e-01, -1.2958e-01, -6.0863e-01],\n",
      "         [-4.1518e-01, -6.3282e-02,  1.3802e+00, -2.9751e-01, -1.7876e+00,\n",
      "          -6.7966e-01,  4.6389e-01,  1.3992e+00],\n",
      "         [-2.2206e-01,  7.6023e-01,  7.6787e-02, -9.9527e-01,  9.1406e-01,\n",
      "           8.7319e-01, -2.0798e+00,  6.7286e-01],\n",
      "         [-9.6597e-01,  4.8302e-01,  5.2730e-01,  7.1540e-01, -6.9237e-02,\n",
      "          -2.2079e+00,  8.3840e-01,  6.7903e-01],\n",
      "         [ 1.1127e+00, -2.1477e+00,  1.3451e+00, -2.1811e-01, -3.0271e-02,\n",
      "           3.5691e-01, -1.3871e-02, -4.0487e-01],\n",
      "         [-3.3535e-01, -1.3941e+00,  2.1066e+00, -3.8259e-01,  5.1786e-01,\n",
      "          -6.8951e-01, -4.5943e-01,  6.3657e-01]]], device='mps:0')\n",
      "====================Exiting Norm.LayerNorm====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.208/2.360\n",
      "Full tensor content:\n",
      "tensor([[[ 1.6111e+00,  1.5102e+00, -3.6867e-01, -2.6119e-02, -1.5301e+00,\n",
      "          -6.4529e-01, -7.8669e-02, -4.7247e-01],\n",
      "         [ 7.2265e-01, -1.5199e-01, -1.6025e+00,  1.1990e+00, -6.8164e-01,\n",
      "          -5.9765e-01, -4.4616e-01,  1.5583e+00],\n",
      "         [-8.2648e-01,  3.5433e-01, -1.2174e+00, -6.6718e-01,  1.5671e+00,\n",
      "          -6.1196e-01, -1.5120e-01,  1.5527e+00],\n",
      "         [ 9.2433e-01, -1.9646e-01, -3.6129e-01, -2.9759e-01, -9.0440e-01,\n",
      "          -1.4681e+00,  3.5750e-01,  1.9460e+00],\n",
      "         [ 4.7846e-01,  2.0785e+00, -1.6363e+00, -1.7674e-01, -3.2842e-01,\n",
      "           4.8134e-01, -4.6091e-01, -4.3598e-01],\n",
      "         [ 4.5061e-02,  4.7226e-01, -8.0632e-01, -9.8635e-01, -4.9328e-01,\n",
      "          -8.9935e-01,  2.2117e+00,  4.5631e-01],\n",
      "         [-6.0214e-02,  7.3958e-01,  4.9173e-01, -4.6907e-01,  9.5866e-01,\n",
      "           1.2123e+00, -9.5075e-01, -1.9222e+00],\n",
      "         [ 2.6705e-01, -1.4734e+00, -1.6124e+00,  7.7779e-01,  7.9804e-01,\n",
      "          -3.9241e-01,  3.5659e-01,  1.2787e+00],\n",
      "         [ 1.5256e-01,  4.6222e-01, -1.2307e+00, -8.9884e-01, -1.2396e+00,\n",
      "           7.3754e-01,  1.9413e-01,  1.8227e+00],\n",
      "         [-1.5996e+00, -5.5221e-01, -8.1570e-02,  3.9360e-01,  3.7196e-01,\n",
      "           1.1962e+00,  1.4336e+00, -1.1620e+00]],\n",
      "\n",
      "        [[-2.7327e-02,  1.5771e-01, -1.5275e+00,  6.8642e-01,  1.8094e+00,\n",
      "           1.8849e-01,  7.4785e-02, -1.3620e+00],\n",
      "         [ 9.1636e-01,  1.0467e+00, -1.5793e+00, -1.5689e+00,  8.4356e-01,\n",
      "          -3.0507e-01,  5.4186e-01,  1.0471e-01],\n",
      "         [-5.6364e-01, -1.0434e+00,  1.0200e+00,  3.0044e-02, -1.2091e+00,\n",
      "           8.0410e-01,  1.7019e+00, -7.3985e-01],\n",
      "         [-5.1831e-01, -3.2400e-01, -5.6071e-01,  2.3596e+00, -5.5762e-01,\n",
      "          -1.0218e+00, -1.0813e-03,  6.2399e-01],\n",
      "         [-3.7606e-01,  1.9341e+00,  8.8381e-01, -4.0966e-01, -2.7921e-01,\n",
      "           5.5087e-01, -8.9373e-01, -1.4102e+00],\n",
      "         [ 9.2323e-01, -8.0907e-01, -1.5873e+00, -1.9133e-02,  4.4330e-01,\n",
      "           9.3343e-01,  1.2616e+00, -1.1461e+00],\n",
      "         [-8.1855e-01,  6.5396e-01, -9.2419e-01,  1.8285e+00, -1.4485e-01,\n",
      "          -9.1203e-01, -7.9056e-01,  1.1078e+00],\n",
      "         [ 1.6864e-01, -7.1432e-02,  7.0453e-01,  6.4434e-01, -8.1599e-01,\n",
      "          -2.0789e+00,  1.1180e-02,  1.4377e+00],\n",
      "         [ 6.0656e-01,  1.2862e+00,  4.4143e-01, -1.4291e+00,  4.5853e-01,\n",
      "          -1.8363e+00,  8.3470e-02,  3.8925e-01],\n",
      "         [ 2.6412e-01, -5.3318e-01, -5.7006e-02, -5.2249e-01, -1.6030e+00,\n",
      "           1.9012e+00, -4.4387e-01,  9.9422e-01]],\n",
      "\n",
      "        [[-9.1009e-01,  1.1841e+00, -1.8615e+00,  3.5202e-01,  7.0845e-01,\n",
      "          -2.2730e-01,  1.1964e+00, -4.4211e-01],\n",
      "         [ 4.5444e-01,  1.3611e+00, -5.2070e-02, -1.3146e+00,  6.3341e-02,\n",
      "          -2.6997e-01, -1.5535e+00,  1.3113e+00],\n",
      "         [ 2.3931e-01, -6.8556e-01,  1.3273e+00, -1.0723e+00, -2.6580e-01,\n",
      "           4.0728e-01, -1.4454e+00,  1.4952e+00],\n",
      "         [ 6.5820e-01,  7.7018e-01, -4.4526e-01,  8.5433e-01, -1.0286e+00,\n",
      "           9.7351e-01,  2.1590e-01, -1.9983e+00],\n",
      "         [-1.0903e-01,  1.1901e+00, -7.5586e-01, -4.8160e-01, -4.1198e-01,\n",
      "          -2.6873e-01,  2.0273e+00, -1.1902e+00],\n",
      "         [-1.3795e+00,  8.1876e-01,  8.3967e-01, -1.0241e+00,  1.4969e-02,\n",
      "          -5.8254e-01, -4.5526e-01,  1.7680e+00],\n",
      "         [-9.2314e-01, -3.6483e-02,  1.5321e+00,  2.7430e-01, -6.2384e-02,\n",
      "          -1.4518e+00,  1.4268e+00, -7.5937e-01],\n",
      "         [ 1.1984e+00,  1.1358e+00,  8.3714e-02, -1.2448e+00,  7.1792e-01,\n",
      "          -6.8196e-01,  3.9703e-01, -1.6060e+00],\n",
      "         [ 4.5543e-01,  4.7890e-01,  9.3351e-01, -1.3576e+00, -1.2332e+00,\n",
      "           1.5901e-01,  1.5357e+00, -9.7174e-01],\n",
      "         [-8.8444e-01,  1.3745e+00, -1.7325e+00,  9.8504e-02, -4.5944e-01,\n",
      "           4.3795e-01, -2.0317e-01,  1.3686e+00]],\n",
      "\n",
      "        [[ 3.7880e-02,  1.4647e+00, -9.9721e-01,  5.9185e-01, -2.9754e-01,\n",
      "          -7.1879e-01, -1.4369e+00,  1.3559e+00],\n",
      "         [-1.0242e+00, -4.4049e-01, -1.6146e+00,  6.8863e-01,  1.5241e+00,\n",
      "          -1.4072e-01,  1.1462e+00, -1.3906e-01],\n",
      "         [-9.4205e-01,  2.4916e-01, -2.6226e-01, -1.9204e+00,  1.5456e+00,\n",
      "           8.0067e-01,  5.1336e-01,  1.5872e-02],\n",
      "         [-9.8502e-01,  1.8559e-01,  1.8244e+00, -1.5157e+00, -4.0461e-02,\n",
      "           6.0168e-01,  6.7304e-01, -7.4347e-01],\n",
      "         [ 9.7255e-01, -1.1661e+00,  1.3314e+00, -1.3460e+00,  1.2717e+00,\n",
      "          -3.2532e-01, -1.2958e-01, -6.0863e-01],\n",
      "         [-4.1518e-01, -6.3282e-02,  1.3802e+00, -2.9751e-01, -1.7876e+00,\n",
      "          -6.7966e-01,  4.6389e-01,  1.3992e+00],\n",
      "         [-2.2206e-01,  7.6023e-01,  7.6787e-02, -9.9527e-01,  9.1406e-01,\n",
      "           8.7319e-01, -2.0798e+00,  6.7286e-01],\n",
      "         [-9.6597e-01,  4.8302e-01,  5.2730e-01,  7.1540e-01, -6.9237e-02,\n",
      "          -2.2079e+00,  8.3840e-01,  6.7903e-01],\n",
      "         [ 1.1127e+00, -2.1477e+00,  1.3451e+00, -2.1811e-01, -3.0271e-02,\n",
      "           3.5691e-01, -1.3871e-02, -4.0487e-01],\n",
      "         [-3.3535e-01, -1.3941e+00,  2.1066e+00, -3.8259e-01,  5.1786e-01,\n",
      "          -6.8951e-01, -4.5943e-01,  6.3657e-01]]], device='mps:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "====================Exiting Norm.forward====================\n",
      "CPU times: user 120 ms, sys: 87.4 ms, total: 208 ms\n",
      "Wall time: 382 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# LayerNorm\n",
    "module = Norm(cfg.dim, 'LayerNorm').to(cfg.device)\n",
    "module.enable_logging()\n",
    "\n",
    "# you can also have it optionally print out all tensors in full\n",
    "module.enable_full_tensor_printing()\n",
    "# i recommend only doing this with very small toy values for your hyperparameters, otherwise this gets too big\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('CosineNorm')\n",
    "#module.disable_function_logging('LayerNorm')\n",
    "#module.disable_function_logging('RMSNorm')\n",
    "\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "output = module(x)\n",
    "module.disable_logging()\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e190f3de-37fd-442b-bfb1-6a090115fc75",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a89e1e8f-cedd-4885-ad50-934827ed045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.attention import SelfAttention, PrecomputeRotaryFrequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e8f27b4-6d1f-4fcd-99b0-2284e65d6b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.19K\n",
      "SelfAttention(\n",
      "  (Wq): Linear(in_features=8, out_features=8, bias=False)\n",
      "  (Wk): Linear(in_features=8, out_features=4, bias=False)\n",
      "  (Wv): Linear(in_features=8, out_features=4, bias=False)\n",
      "  (Wo): Linear(in_features=8, out_features=8, bias=False)\n",
      ")\n",
      "\n",
      "====================Entering SelfAttention.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.691/3.249\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.959/0.989\n",
      "Tensor 'mask' shape: torch.Size([10, 10]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering SelfAttention.apply_rotary_pos_emb====================\n",
      "Inputs:\n",
      "Tensor 'q' shape: torch.Size([4, 10, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.522/1.880\n",
      "Tensor 'k' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.288/1.377\n",
      "Dict 'freqs_cis':\n",
      "    Tensor 'freqs_cis[cos]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs_cis[sin]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.959/0.989\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "\n",
      "====================Entering SelfAttention.rotate_half====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.522/1.880\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.880/1.796\n",
      "====================Exiting SelfAttention.rotate_half====================\n",
      "\n",
      "====================Entering SelfAttention.rotate_half====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.288/1.377\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.288/1.377\n",
      "====================Exiting SelfAttention.rotate_half====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 10, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.395/1.880\n",
      "Tensor 'output[1]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.312/1.457\n",
      "====================Exiting SelfAttention.apply_rotary_pos_emb====================\n",
      "\n",
      "====================Entering SelfAttention.match_headcount====================\n",
      "Inputs:\n",
      "Tensor 'k' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.312/1.457\n",
      "Tensor 'v' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.215/1.970\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 10, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.312/1.457\n",
      "Tensor 'output[1]' shape: torch.Size([4, 10, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.215/1.970\n",
      "====================Exiting SelfAttention.match_headcount====================\n",
      "\n",
      "====================Entering SelfAttention.flash_attention====================\n",
      "Inputs:\n",
      "Tensor 'q' shape: torch.Size([4, 2, 10, 4]), dtype: torch.float32, device: mps:0, min/max: -1.395/1.880\n",
      "Tensor 'k' shape: torch.Size([4, 2, 10, 4]), dtype: torch.float32, device: mps:0, min/max: -1.312/1.457\n",
      "Tensor 'v' shape: torch.Size([4, 2, 10, 4]), dtype: torch.float32, device: mps:0, min/max: -1.215/1.970\n",
      "Tensor 'mask' shape: torch.Size([10, 10]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 2, 10, 4]), dtype: torch.float32, device: mps:0, min/max: -1.350/1.152\n",
      "====================Exiting SelfAttention.flash_attention====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.950/1.126\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting SelfAttention.forward====================\n",
      "CPU times: user 211 ms, sys: 25.7 ms, total: 237 ms\n",
      "Wall time: 251 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# first up let's look at training\n",
    "\n",
    "# Create an instance of multi-head self-attention\n",
    "module = SelfAttention(cfg.dim, cfg.head_dim, cfg.num_q_heads, cfg.num_kv_heads, cfg.max_seq_len, cfg.linear_bias, device=cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "\n",
    "# Initially, logging is disabled by default\n",
    "module.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('apply_precompute_freqs')\n",
    "#module.disable_function_logging('reshape_for_broadcast')\n",
    "#module.disable_function_logging('match_headcount')\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('calc_output')\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "precompute_freqs = PrecomputeRotaryFrequencies(cfg.head_dim, cfg.max_seq_len, cfg.theta, cfg.device)\n",
    "freqs = precompute_freqs()\n",
    "mask = torch.ones(cfg.max_seq_len, cfg.max_seq_len, dtype=torch.bool, device=cfg.device).triu(diagonal=1)\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "output = module(x, freqs, mask, training=True)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, freqs, mask, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7e034c-c5a6-43ac-b6d2-bbb71f9965ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================Entering SelfAttention.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 2, 8]), dtype: torch.float32, device: mps:0, min/max: -2.401/2.478\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.959/0.989\n",
      "Tensor 'mask' shape: torch.Size([2, 5]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Integer 'cache_len': Value=3\n",
      "Dict 'kv_cache':\n",
      "    Tensor 'kv_cache[k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Tensor 'kv_cache[v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "Bool 'training': Value=False\n",
      "\n",
      "====================Entering SelfAttention.apply_rotary_pos_emb====================\n",
      "Inputs:\n",
      "Tensor 'q' shape: torch.Size([4, 2, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.750/1.798\n",
      "Tensor 'k' shape: torch.Size([4, 2, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.492/1.116\n",
      "Dict 'freqs_cis':\n",
      "    Tensor 'freqs_cis[cos]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs_cis[sin]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.959/0.989\n",
      "Integer 'cache_len': Value=3\n",
      "\n",
      "====================Entering SelfAttention.rotate_half====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 2, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.750/1.798\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 2, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.798/1.750\n",
      "====================Exiting SelfAttention.rotate_half====================\n",
      "\n",
      "====================Entering SelfAttention.rotate_half====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 2, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.492/1.116\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 2, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.492/1.116\n",
      "====================Exiting SelfAttention.rotate_half====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 2, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.724/1.790\n",
      "Tensor 'output[1]' shape: torch.Size([4, 2, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.491/0.999\n",
      "====================Exiting SelfAttention.apply_rotary_pos_emb====================\n",
      "\n",
      "====================Entering SelfAttention.match_headcount====================\n",
      "Inputs:\n",
      "Tensor 'k' shape: torch.Size([4, 5, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.491/0.999\n",
      "Tensor 'v' shape: torch.Size([4, 5, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.965/0.738\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 5, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.491/0.999\n",
      "Tensor 'output[1]' shape: torch.Size([4, 5, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -0.965/0.738\n",
      "====================Exiting SelfAttention.match_headcount====================\n",
      "\n",
      "====================Entering SelfAttention.flash_attention====================\n",
      "Inputs:\n",
      "Tensor 'q' shape: torch.Size([4, 2, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.724/1.790\n",
      "Tensor 'k' shape: torch.Size([4, 2, 5, 4]), dtype: torch.float32, device: mps:0, min/max: -1.491/0.999\n",
      "Tensor 'v' shape: torch.Size([4, 2, 5, 4]), dtype: torch.float32, device: mps:0, min/max: -0.965/0.738\n",
      "Tensor 'mask' shape: torch.Size([2, 5]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Bool 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 2, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -0.290/0.296\n",
      "====================Exiting SelfAttention.flash_attention====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 2, 8]), dtype: torch.float32, device: mps:0, min/max: -0.182/0.211\n",
      "Dict 'output[1]':\n",
      "    Tensor 'output[1][k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.491/0.999\n",
      "    Tensor 'output[1][v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.965/0.738\n",
      "====================Exiting SelfAttention.forward====================\n",
      "CPU times: user 126 ms, sys: 15.8 ms, total: 142 ms\n",
      "Wall time: 155 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# now let's do it for inference\n",
    "\n",
    "module = SelfAttention(cfg.dim, cfg.head_dim, cfg.num_q_heads, cfg.num_kv_heads, cfg.max_seq_len, cfg.linear_bias, device=cfg.device)\n",
    "module.enable_logging()\n",
    "#module.disable_function_logging('apply_precompute_freqs')\n",
    "#module.disable_function_logging('reshape_for_broadcast')\n",
    "#module.disable_function_logging('match_headcount')\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('calc_output')\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "precompute_freqs = PrecomputeRotaryFrequencies(cfg.head_dim, cfg.max_seq_len, cfg.theta, cfg.device)\n",
    "freqs_cis = precompute_freqs()\n",
    "mask = torch.ones(cfg.max_seq_len, cfg.max_seq_len, dtype=torch.bool, device=cfg.device).triu(diagonal=1)\n",
    "# setting up for kv caching\n",
    "context_chunk_len = cfg.max_seq_len // 4\n",
    "cache_len = random.randint(1, 3 * context_chunk_len)\n",
    "seq_len = cache_len + context_chunk_len\n",
    "kv_cache = {\n",
    "    'k': torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device),\n",
    "    'v': torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device)\n",
    "}\n",
    "# need to extend the mask with zeros for the cached values\n",
    "mask = torch.nn.functional.pad(mask[:context_chunk_len, :context_chunk_len], (cache_len, 0, 0, 0), value=False).bool()\n",
    "x = torch.randn(tcfg.micro_batch_size,context_chunk_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "output = module(x, freqs_cis, mask, cache_len, kv_cache)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, freqs_cis, mask, cache_len, context_chunk_len, seq_len, kv_cache, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb308c5-b578-46f2-86ae-bfa6800be641",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7736b685-f941-4182-a5b7-4731cce706b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.mlp import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c62e49e9-2189-4269-968e-1df99469dc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.50K\n",
      "MLP(\n",
      "  (Wup): Linear(in_features=8, out_features=21, bias=False)\n",
      "  (Wgate): Linear(in_features=8, out_features=21, bias=False)\n",
      "  (Wdown): Linear(in_features=21, out_features=8, bias=False)\n",
      "  (nonlinearity): GELU(approximate='none')\n",
      ")\n",
      "\n",
      "====================Entering MLP.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.388/2.765\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.461/0.734\n",
      "====================Exiting MLP.forward====================\n",
      "CPU times: user 26.1 ms, sys: 3.76 ms, total: 29.8 ms\n",
      "Wall time: 30.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# GeGLU\n",
    "module = MLP(\n",
    "    cfg.dim, \n",
    "    int(cfg.dim * cfg.mlp_hidden_mult * 2/3), \n",
    "    cfg.dim, \n",
    "    'GeLU', \n",
    "    gated=True, \n",
    "    bias=cfg.linear_bias, \n",
    "    dropout_rate = 0.1\n",
    ").to(cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "module.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "output = module(x, training=True)\n",
    "module.disable_logging()\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0748fa3-3230-4dd7-a768-1256ea72e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.55K\n",
      "MLP(\n",
      "  (Wup): Linear(in_features=8, out_features=32, bias=True)\n",
      "  (Wdown): Linear(in_features=32, out_features=8, bias=True)\n",
      "  (nonlinearity): ReLU()\n",
      ")\n",
      "\n",
      "====================Entering MLP.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -3.319/2.575\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.687/1.157\n",
      "====================Exiting MLP.forward====================\n",
      "CPU times: user 27.3 ms, sys: 3.71 ms, total: 31 ms\n",
      "Wall time: 30.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# not gated, testing every other nonlinearity\n",
    "module = MLP(\n",
    "    cfg.dim, \n",
    "    cfg.dim * cfg.mlp_hidden_mult, \n",
    "    cfg.dim, \n",
    "    'ReLU', \n",
    "    gated=False, \n",
    "    bias=True, \n",
    "    dropout_rate = 0.1\n",
    ").to(cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "module.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "output = module(x, training=True)\n",
    "module.disable_logging()\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3a502f-4646-4a02-9412-372482af9fa0",
   "metadata": {},
   "source": [
    "# ResidualLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a21d708-af47-4f32-b111-08efedc584f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.layer import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c886661d-5a26-4787-93d1-c3c8c6b1c02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.73K\n",
      "Layer(\n",
      "  (pre_attn_norm): Norm()\n",
      "  (attn): SelfAttention(\n",
      "    (Wq): Linear(in_features=8, out_features=8, bias=False)\n",
      "    (Wk): Linear(in_features=8, out_features=4, bias=False)\n",
      "    (Wv): Linear(in_features=8, out_features=4, bias=False)\n",
      "    (Wo): Linear(in_features=8, out_features=8, bias=False)\n",
      "  )\n",
      "  (pre_mlp_norm): Norm()\n",
      "  (mlp): MLP(\n",
      "    (Wup): Linear(in_features=8, out_features=21, bias=False)\n",
      "    (Wgate): Linear(in_features=8, out_features=21, bias=False)\n",
      "    (Wdown): Linear(in_features=21, out_features=8, bias=False)\n",
      "    (nonlinearity): SiLU()\n",
      "  )\n",
      ")\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.756/2.746\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.959/0.989\n",
      "Tensor 'mask' shape: torch.Size([10, 10]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.756/2.746\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.959/0.989\n",
      "Tensor 'mask' shape: torch.Size([10, 10]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.688/0.557\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.729/2.480\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.327/0.382\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.595/2.458\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.forward====================\n",
      "CPU times: user 25.4 ms, sys: 4.9 ms, total: 30.3 ms\n",
      "Wall time: 33.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TRAINING\n",
    "module = Layer(cfg).to(cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "#module.disable_function_logging('attn_connect')\n",
    "#module.disable_function_logging('mlp_connect')\n",
    "### enabling printing for sub-modules\n",
    "#module.pre_attn_norm.enable_logging()\n",
    "#module.attn.enable_logging()\n",
    "#module.post_attn_norm.enable_logging()\n",
    "#module.pre_mlp_norm.enable_logging()\n",
    "#module.mlp.enable_logging()\n",
    "#module.post_mlp_norm.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "precompute_freqs = PrecomputeRotaryFrequencies(cfg.head_dim, cfg.max_seq_len, cfg.theta, cfg.device)\n",
    "freqs_cis = precompute_freqs()\n",
    "mask = torch.ones(cfg.max_seq_len, cfg.max_seq_len, dtype=torch.bool, device=cfg.device).triu(diagonal=1)\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "output = module(x, freqs_cis, mask, training=True)\n",
    "module.disable_logging()\n",
    "del module,freqs_cis, mask, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f534ab4-a150-42db-8c89-5e8d3d0c06a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.73K\n",
      "Layer(\n",
      "  (pre_attn_norm): Norm()\n",
      "  (attn): SelfAttention(\n",
      "    (Wq): Linear(in_features=8, out_features=8, bias=False)\n",
      "    (Wk): Linear(in_features=8, out_features=4, bias=False)\n",
      "    (Wv): Linear(in_features=8, out_features=4, bias=False)\n",
      "    (Wo): Linear(in_features=8, out_features=8, bias=False)\n",
      "  )\n",
      "  (pre_mlp_norm): Norm()\n",
      "  (mlp): MLP(\n",
      "    (Wup): Linear(in_features=8, out_features=21, bias=False)\n",
      "    (Wgate): Linear(in_features=8, out_features=21, bias=False)\n",
      "    (Wdown): Linear(in_features=21, out_features=8, bias=False)\n",
      "    (nonlinearity): SiLU()\n",
      "  )\n",
      ")\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 2, 8]), dtype: torch.float32, device: mps:0, min/max: -1.830/2.133\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.959/0.989\n",
      "Tensor 'mask' shape: torch.Size([2, 5]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Integer 'cache_len': Value=3\n",
      "Dict 'kv_cache':\n",
      "    Tensor 'kv_cache[k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Tensor 'kv_cache[v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "Bool 'training': Value=False\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 2, 8]), dtype: torch.float32, device: mps:0, min/max: -1.830/2.133\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.959/0.989\n",
      "Tensor 'mask' shape: torch.Size([2, 5]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Integer 'cache_len': Value=3\n",
      "Dict 'kv_cache':\n",
      "    Tensor 'kv_cache[k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Tensor 'kv_cache[v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "Bool 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 2, 8]), dtype: torch.float32, device: mps:0, min/max: -0.218/0.217\n",
      "Dict 'output[1]':\n",
      "    Tensor 'output[1][k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.100/1.089\n",
      "    Tensor 'output[1][v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.225/1.310\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 2, 8]), dtype: torch.float32, device: mps:0, min/max: -1.864/2.145\n",
      "Bool 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 2, 8]), dtype: torch.float32, device: mps:0, min/max: -0.540/0.205\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 2, 8]), dtype: torch.float32, device: mps:0, min/max: -1.770/2.319\n",
      "Dict 'output[1]':\n",
      "    Tensor 'output[1][k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.100/1.089\n",
      "    Tensor 'output[1][v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.225/1.310\n",
      "====================Exiting Layer.forward====================\n",
      "CPU times: user 54.5 ms, sys: 7.97 ms, total: 62.4 ms\n",
      "Wall time: 67.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# INFERENCE\n",
    "module = Layer(cfg).to(cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "#module.disable_function_logging('attn_connect')\n",
    "#module.disable_function_logging('mlp_connect')\n",
    "#module.pre_attn_norm.enable_logging()\n",
    "#module.attn.enable_logging()\n",
    "#module.post_attn_norm.enable_logging()\n",
    "#module.pre_mlp_norm.enable_logging()\n",
    "#module.mlp.enable_logging()\n",
    "#module.post_mlp_norm.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "precompute_freqs = PrecomputeRotaryFrequencies(cfg.head_dim, cfg.max_seq_len, cfg.theta, cfg.device)\n",
    "freqs_cis = precompute_freqs()\n",
    "mask = torch.ones(cfg.max_seq_len, cfg.max_seq_len, dtype=torch.bool, device=cfg.device).triu(diagonal=1)\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "# setting up for kv caching\n",
    "cache_len = cfg.max_seq_len // 3\n",
    "context_chunk_len = cfg.max_seq_len // 4\n",
    "seq_len = cache_len + context_chunk_len\n",
    "kv_cache = {\n",
    "    'k': torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device),\n",
    "    'v': torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device)\n",
    "}\n",
    "# need to extend the mask with zeros for the cached values\n",
    "mask = torch.nn.functional.pad(mask[:context_chunk_len, :context_chunk_len], (cache_len, 0, 0, 0), value=False).bool()\n",
    "# these don't use seq_len because those entries should already be in the kv cache\n",
    "#freqs_cis = freqs_cis[:context_chunk_len]\n",
    "x = torch.randn(tcfg.micro_batch_size,context_chunk_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "output = module(x, freqs_cis, mask, cache_len, kv_cache)\n",
    "module.disable_logging()\n",
    "del module, freqs_cis, mask, cache_len, context_chunk_len, seq_len, kv_cache, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c677ac2b-06d0-4895-b718-2bc664613c98",
   "metadata": {},
   "source": [
    "# Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "655a1fec-4e32-4c7a-86e0-1390a03e88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ccc88d4-c650-43d3-85ee-e68152cb2e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.01M\n",
      "Model(\n",
      "  (token_embedder): Embedding(512, 8)\n",
      "  (precompute_freqs): PrecomputeRotaryFrequencies()\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): SelfAttention(\n",
      "        (Wq): Linear(in_features=8, out_features=8, bias=False)\n",
      "        (Wk): Linear(in_features=8, out_features=4, bias=False)\n",
      "        (Wv): Linear(in_features=8, out_features=4, bias=False)\n",
      "        (Wo): Linear(in_features=8, out_features=8, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wup): Linear(in_features=8, out_features=21, bias=False)\n",
      "        (Wgate): Linear(in_features=8, out_features=21, bias=False)\n",
      "        (Wdown): Linear(in_features=21, out_features=8, bias=False)\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (output): Linear(in_features=8, out_features=512, bias=False)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "\n",
      "====================Entering Model.forward====================\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([4, 10]), dtype: torch.int64, device: mps:0, min/max: 3.000/511.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Tensor 'target_token_ids' shape: torch.Size([4, 10]), dtype: torch.int64, device: mps:0, min/max: 6.000/503.000\n",
      "\n",
      "====================Entering PrecomputeRotaryFrequencies.forward====================\n",
      "Inputs:\n",
      "\n",
      "Outputs:\n",
      "Dict 'output':\n",
      "    Tensor 'output[cos]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'output[sin]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.959/0.989\n",
      "====================Exiting PrecomputeRotaryFrequencies.forward====================\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.154/0.138\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.959/0.989\n",
      "Tensor 'mask' shape: torch.Size([10, 10]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.154/0.138\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.959/0.989\n",
      "Tensor 'mask' shape: torch.Size([10, 10]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.004/0.004\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.154/0.138\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.000/0.000\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.154/0.138\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.forward====================\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.154/0.138\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.959/0.989\n",
      "Tensor 'mask' shape: torch.Size([10, 10]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.154/0.138\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.959/0.989\n",
      "Tensor 'mask' shape: torch.Size([10, 10]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.003/0.004\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.154/0.138\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.000/0.000\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.154/0.138\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.forward====================\n",
      "\n",
      "====================Entering Norm.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.154/0.138\n",
      "\n",
      "====================Entering Norm.RMSNorm====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -0.154/0.138\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.332/2.587\n",
      "====================Exiting Norm.RMSNorm====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([4, 10, 8]), dtype: torch.float32, device: mps:0, min/max: -2.332/2.587\n",
      "====================Exiting Norm.forward====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 10, 512]), dtype: torch.float32, device: mps:0, min/max: -0.211/0.238\n",
      "Tensor 'output[1]' shape: torch.Size([]), dtype: torch.float32, device: mps:0, min/max: 6.224/6.224\n",
      "====================Exiting Model.forward====================\n",
      "tensor(6.2238, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "CPU times: user 161 ms, sys: 18.7 ms, total: 179 ms\n",
      "Wall time: 193 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TRAINING\n",
    "module = Model(cfg).to(cfg.device)\n",
    "print(\"number of parameters: %.2fM\" % (module.get_num_params()/1e6,))\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "### enabling printing for sub-modules\n",
    "module.precompute_freqs.enable_logging()\n",
    "#module.layers[0].enable_logging()\n",
    "for i in range(cfg.num_layers):\n",
    "    module.layers[i].enable_logging()\n",
    "module.final_norm.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "input_token_ids = torch.randint(tokenizer.vocab_len, (tcfg.micro_batch_size, cfg.max_seq_len)).to(cfg.device)\n",
    "target_token_ids = torch.randint(tokenizer.vocab_len, (tcfg.micro_batch_size, cfg.max_seq_len)).to(cfg.device)\n",
    "\n",
    "output, loss = module(input_token_ids, target_token_ids=target_token_ids)\n",
    "print(loss)\n",
    "del module, input_token_ids, target_token_ids, output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "316598a6-1ba9-4a26-962f-16c0802a698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.01M\n",
      "Model(\n",
      "  (token_embedder): Embedding(512, 8)\n",
      "  (precompute_freqs): PrecomputeRotaryFrequencies()\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): SelfAttention(\n",
      "        (Wq): Linear(in_features=8, out_features=8, bias=False)\n",
      "        (Wk): Linear(in_features=8, out_features=4, bias=False)\n",
      "        (Wv): Linear(in_features=8, out_features=4, bias=False)\n",
      "        (Wo): Linear(in_features=8, out_features=8, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wup): Linear(in_features=8, out_features=21, bias=False)\n",
      "        (Wgate): Linear(in_features=8, out_features=21, bias=False)\n",
      "        (Wdown): Linear(in_features=21, out_features=8, bias=False)\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (output): Linear(in_features=8, out_features=512, bias=False)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "\n",
      "====================Entering Model.forward====================\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([4, 2]), dtype: torch.int64, device: mps:0, min/max: 56.000/494.000\n",
      "Integer 'cache_len': Value=3\n",
      "List 'kv_cache':\n",
      "    Dict 'kv_cache[0]':\n",
      "        Tensor 'kv_cache[0][k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "        Tensor 'kv_cache[0][v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Dict 'kv_cache[1]':\n",
      "        Tensor 'kv_cache[1][k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "        Tensor 'kv_cache[1][v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "Other-type 'target_token_ids': Type=NoneType, Value=None\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([4, 2, 512]), dtype: torch.float32, device: mps:0, min/max: -0.190/0.193\n",
      "List 'output[1]':\n",
      "    Dict 'output[1][0]':\n",
      "        Tensor 'output[1][0][k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.084/0.123\n",
      "        Tensor 'output[1][0][v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.132/0.149\n",
      "    Dict 'output[1][1]':\n",
      "        Tensor 'output[1][1][k]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.170/0.124\n",
      "        Tensor 'output[1][1][v]' shape: torch.Size([4, 10, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.184/0.181\n",
      "====================Exiting Model.forward====================\n",
      "CPU times: user 41.7 ms, sys: 6.29 ms, total: 48 ms\n",
      "Wall time: 50.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Inference\n",
    "module = Model(cfg).to(cfg.device)\n",
    "print(\"number of parameters: %.2fM\" % (module.get_num_params()/1e6,))\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "### enabling printing for sub-modules\n",
    "#for i in range(cfg.num_layers):\n",
    "    #module.layers[i].enable_logging()\n",
    "#module.final_norm.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "input_token_ids = torch.randint(tokenizer.vocab_len, (tcfg.micro_batch_size, cfg.max_seq_len // 4)).to(cfg.device)\n",
    "kv_cache = [{ # Initialize kv caches for each layer\n",
    "                \"k\": torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device),\n",
    "                \"v\": torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device),\n",
    "            } for _ in range(cfg.num_layers)]\n",
    "\n",
    "output, kv_cache = module(input_token_ids, cache_len = cfg.max_seq_len // 3, kv_cache = kv_cache)\n",
    "\n",
    "del module, input_token_ids, kv_cache, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d82f9c-e531-47b5-8ffb-c8374ffb0e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
